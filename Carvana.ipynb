{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carvana U-Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from keras.optimizers import Adam\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TensorBoard\n",
    "from keras.layers import AveragePooling2D\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from optimizers.AdamAccumulate import AdamAccumulate\n",
    "from models.u_net import UNet\n",
    "from submit import generate_submit\n",
    "import utils\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 128\n",
    "train_path = \"inputs/train/{}.jpg\" \n",
    "train_mask_path = \"inputs/train_masks/{}_mask.gif\"\n",
    "df_train = pd.read_csv('inputs/train_masks.csv')\n",
    "ids_train = df_train['img'].map(lambda s: s.split('.')[0])#[:3000]\n",
    "ids_train_split, ids_valid_split = train_test_split(ids_train, test_size=0.2, random_state=42)\n",
    "\n",
    "print('Training on {} samples'.format(len(ids_train_split)))\n",
    "print('Validating on {} samples'.format(len(ids_valid_split)))\n",
    "\n",
    "bboxes = None\n",
    "bbox_file_path = 'inputs/train_bbox.csv'\n",
    "bboxes = utils.get_bboxes(bbox_file_path)\n",
    "\n",
    "def train_generator(batch_size):\n",
    "    return utils.train_generator(train_path, train_mask_path, ids_train_split, input_size, batch_size, bboxes)\n",
    "\n",
    "def valid_generator(batch_size):\n",
    "    return utils.valid_generator(train_path, train_mask_path, ids_valid_split, input_size, batch_size, bboxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_value(y_true, y_pred):\n",
    "    smooth = 1.\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "#def bce_dice_loss(y_true, y_pred):\n",
    "#    return binary_crossentropy(y_true, y_pred) + (1 - dice_value(y_true, y_pred))\n",
    "\n",
    "def weighted_bce_loss(y_true, y_pred, weights):\n",
    "    return K.mean(tf.nn.weighted_cross_entropy_with_logits(y_true, y_pred, weights), axis=-1)\n",
    "\n",
    "def weighted_dice_value(y_true, y_pred, weights):\n",
    "    smooth = 1.\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = y_true_f * y_pred_f\n",
    "    weights = K.flatten(weights)\n",
    "    w2 = weights * weights\n",
    "    return (2. * K.sum(w2 * intersection) + smooth) / (K.sum(w2 * y_true_f) + K.sum(w2 * y_pred_f) + smooth)\n",
    "\n",
    "def weighted_bce_dice_loss(y_true, y_pred):\n",
    "    a = AveragePooling2D(pool_size=(11, 11), strides=1, padding='same')(y_true)\n",
    "    ind = K.cast(K.greater_equal(a, 0.01), 'float32') * K.cast(K.less_equal(a, 0.99), 'float32')\n",
    "    ind = K.cast(ind, 'float32')\n",
    "    weights = K.ones_like(a)\n",
    "    w0 = K.sum(weights)\n",
    "    weights = weights + ind*2\n",
    "    w1 = K.sum(weights)\n",
    "    weights = weights/w1*w0\n",
    "    return  weighted_bce_loss(y_true, y_pred, weights) + (1 - weighted_dice_value(y_true, y_pred, weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FC-DenseNet56:\n",
    "#model = Tiramisu((input_size, input_size, 3), growth_rate=12, depth=5, layers_per_block=[4,4,4,4,4,4]) \n",
    "#FC-DenseNet67:\n",
    "#model = Tiramisu((input_size, input_size, 3), growth_rate=16, depth=5, layers_per_block=[5,5,5,5,5,5]) \n",
    "#FC-DenseNet103:\n",
    "#model = Tiramisu((input_size, input_size, 3), growth_rate=16, depth=5, layers_per_block=[4,5,7,10,12,15]) \n",
    "#FC-DenseNet46 (Not in paper):\n",
    "#model = Tiramisu((input_size, input_size, 3), growth_rate=12, depth=4, layers_per_block=[4,4,4,4,5]) \n",
    "\n",
    "#U-Net:\n",
    "model = UNet((input_size, input_size, 3), \n",
    "             activation=lambda x: PReLU()(x),\n",
    "             dropout_base_only=True,\n",
    "             init='he_uniform')\n",
    "model.compile(optimizer=AdamAccumulate(accum_iters=4), loss=weighted_bce_dice_loss, metrics=[dice_value])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2\n",
    "batch_size = 16\n",
    "run_name = utils.get_run_name('weights/{}.hdf5', 'unet')\n",
    "weights_path = 'weights/{}.hdf5'.format(run_name)\n",
    "\n",
    "callbacks = [EarlyStopping(monitor='val_dice_value',\n",
    "                           patience=8,\n",
    "                           verbose=1,\n",
    "                           min_delta=1e-4,\n",
    "                           mode='max'),\n",
    "             ReduceLROnPlateau(monitor='val_dice_value',\n",
    "                               factor=0.1,\n",
    "                               patience=4,\n",
    "                               verbose=1,\n",
    "                               epsilon=1e-4,\n",
    "                               mode='max'),\n",
    "             ModelCheckpoint(monitor='val_dice_value',\n",
    "                             filepath=weights_path,\n",
    "                             save_best_only=True,\n",
    "                             save_weights_only=True,\n",
    "                             mode='max'),\n",
    "             TensorBoard(log_dir='logs/{}'.format(run_name), batch_size=batch_size)]\n",
    "\n",
    "#model.load_weights('weights/best_weights.hdf5')\n",
    "#K.set_value(model.optimizer.lr, 0.01)\n",
    "\n",
    "print('Starting run \"{}\"'.format(run_name))\n",
    "model.fit_generator(generator=train_generator(batch_size),\n",
    "                    steps_per_epoch=np.ceil(float(len(ids_train_split)) / float(batch_size)),\n",
    "                    epochs=epochs,\n",
    "                    verbose=2,\n",
    "                    callbacks=callbacks,\n",
    "                    validation_data=valid_generator(batch_size),\n",
    "                    validation_steps=np.ceil(float(len(ids_valid_split)) / float(batch_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def np_dice_value(y_true, y_pred):\n",
    "    smooth = 1.\n",
    "    y_true_f = y_true.flatten()\n",
    "    y_pred_f = y_pred.flatten()\n",
    "    intersection = np.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (np.sum(y_true_f) + np.sum(y_pred_f) + smooth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = 'unet-2017-08-20-5'\n",
    "model.load_weights('weights/{}.hdf5'.format(run_name))\n",
    "\n",
    "val_imgs, val_masks = next(valid_generator(len(ids_valid_split)))\n",
    "val_imgs = np.array(val_imgs)\n",
    "val_masks = np.array(val_masks)\n",
    "val_pred_masks = model.predict(val_imgs, batch_size=32)\n",
    "masks_val_dices = [np_dice_value(mask, pred_mask) for (mask, pred_mask) in zip(val_masks, val_pred_masks)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist, bins = np.histogram(masks_val_dices, bins=50)\n",
    "width = 0.7 * (bins[1] - bins[0])\n",
    "center = (bins[:-1] + bins[1:]) / 2\n",
    "plt.bar(center, hist, align='center', width=width)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.random.randint(len(ids_valid_split), size=3)\n",
    "for index in indices:\n",
    "    id = ids_valid_split.values[index]\n",
    "    utils.show_mask(train_path.format(id), val_masks[index].squeeze(), val_pred_masks[index].squeeze(),\n",
    "                    show_img=True, bbox = bboxes[id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model first if required\n",
    "run_name = 'unet-2017-08-20-5'\n",
    "model.load_weights('weights/{}.hdf5'.format(run_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "threshold = 0.5\n",
    "test_path = 'inputs/test1/' #'inputs/test/'\n",
    "test_masks_path = 'outputs/test1_masks/' #None\n",
    "generate_submit(model, input_size, batch_size, threshold, test_path, 'outputs/', run_name, test_masks_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.show_test_masks(test_path, test_masks_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
