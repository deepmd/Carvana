{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carvana U-Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#from keras.optimizers import Adam\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TensorBoard\n",
    "from keras.layers import AveragePooling2D\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from optimizers.AdamAccumulate import AdamAccumulate\n",
    "from models.u_net import UNet\n",
    "from submit import generate_submit\n",
    "import utils\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 4070 samples\n",
      "Validating on 1018 samples\n"
     ]
    }
   ],
   "source": [
    "input_size = 128\n",
    "train_path = \"inputs/train/{}.jpg\" \n",
    "train_mask_path = \"inputs/train_masks/{}_mask.gif\"\n",
    "df_train = pd.read_csv('inputs/train_masks.csv')\n",
    "ids_train = df_train['img'].map(lambda s: s.split('.')[0])#[:3000]\n",
    "ids_train_split, ids_valid_split = train_test_split(ids_train, test_size=0.2, random_state=42)\n",
    "\n",
    "print('Training on {} samples'.format(len(ids_train_split)))\n",
    "print('Validating on {} samples'.format(len(ids_valid_split)))\n",
    "\n",
    "bboxes = None\n",
    "bbox_file_path = 'inputs/train_bbox.csv'\n",
    "bboxes = utils.get_bboxes(bbox_file_path)\n",
    "\n",
    "def train_generator(batch_size):\n",
    "    return utils.train_generator(train_path, train_mask_path, ids_train_split, input_size, batch_size, bboxes)\n",
    "\n",
    "def valid_generator(batch_size):\n",
    "    return utils.valid_generator(train_path, train_mask_path, ids_valid_split, input_size, batch_size, bboxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dice_value(y_true, y_pred):\n",
    "    smooth = 1.\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "#def bce_dice_loss(y_true, y_pred):\n",
    "#    return binary_crossentropy(y_true, y_pred) + (1 - dice_value(y_true, y_pred))\n",
    "\n",
    "def weighted_bce_loss(y_true, y_pred, weights):\n",
    "    return K.mean(tf.nn.weighted_cross_entropy_with_logits(y_true, y_pred, weights), axis=-1)\n",
    "\n",
    "def weighted_dice_value(y_true, y_pred, weights):\n",
    "    smooth = 1.\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = y_true_f * y_pred_f\n",
    "    weights = K.flatten(weights)\n",
    "    w2 = weights * weights\n",
    "    return (2. * K.sum(w2 * intersection) + smooth) / (K.sum(w2 * y_true_f) + K.sum(w2 * y_pred_f) + smooth)\n",
    "\n",
    "def weighted_bce_dice_loss(y_true, y_pred):\n",
    "    a = AveragePooling2D(pool_size=(11, 11), strides=1, padding='same')(y_true)\n",
    "    ind = K.cast(K.greater_equal(a, 0.01), 'float32') * K.cast(K.less_equal(a, 0.99), 'float32')\n",
    "    ind = K.cast(ind, 'float32')\n",
    "    weights = K.ones_like(a)\n",
    "    w0 = K.sum(weights)\n",
    "    weights = weights + ind*2\n",
    "    w1 = K.sum(weights)\n",
    "    weights = weights/w1*w0\n",
    "    return  weighted_bce_loss(y_true, y_pred, weights) + (1 - weighted_dice_value(y_true, y_pred, weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#FC-DenseNet56:\n",
    "#model = Tiramisu((input_size, input_size, 3), growth_rate=12, depth=5, layers_per_block=[4,4,4,4,4,4]) \n",
    "#FC-DenseNet67:\n",
    "#model = Tiramisu((input_size, input_size, 3), growth_rate=16, depth=5, layers_per_block=[5,5,5,5,5,5]) \n",
    "#FC-DenseNet103:\n",
    "#model = Tiramisu((input_size, input_size, 3), growth_rate=16, depth=5, layers_per_block=[4,5,7,10,12,15]) \n",
    "#FC-DenseNet46 (Not in paper):\n",
    "#model = Tiramisu((input_size, input_size, 3), growth_rate=12, depth=4, layers_per_block=[4,4,4,4,5]) \n",
    "\n",
    "#U-Net:\n",
    "model = UNet((input_size, input_size, 3), \n",
    "             activation=lambda x: PReLU()(x),\n",
    "             dropout_base_only=True,\n",
    "             init='he_uniform')\n",
    "model.compile(optimizer=AdamAccumulate(accum_iters=4), loss=weighted_bce_dice_loss, metrics=[dice_value])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "255/255 [==============================] - 158s - loss: 0.5137 - dice_value: 0.9696 - val_loss: 0.5260 - val_dice_value: 0.9643\n",
      "Epoch 2/50\n",
      "255/255 [==============================] - 152s - loss: 0.4704 - dice_value: 0.9872 - val_loss: 0.4584 - val_dice_value: 0.9898\n",
      "Epoch 3/50\n",
      "255/255 [==============================] - 152s - loss: 0.4623 - dice_value: 0.9896 - val_loss: 0.4526 - val_dice_value: 0.9913\n",
      "Epoch 4/50\n",
      "255/255 [==============================] - 153s - loss: 0.4598 - dice_value: 0.9901 - val_loss: 0.4471 - val_dice_value: 0.9928\n",
      "Epoch 5/50\n",
      "255/255 [==============================] - 152s - loss: 0.4576 - dice_value: 0.9908 - val_loss: 0.5131 - val_dice_value: 0.9399\n",
      "Epoch 6/50\n",
      "255/255 [==============================] - 153s - loss: 0.4561 - dice_value: 0.9915 - val_loss: 0.4444 - val_dice_value: 0.9934\n",
      "Epoch 7/50\n",
      "255/255 [==============================] - 153s - loss: 0.4536 - dice_value: 0.9921 - val_loss: 0.4423 - val_dice_value: 0.9940\n",
      "Epoch 8/50\n",
      "255/255 [==============================] - 153s - loss: 0.4531 - dice_value: 0.9922 - val_loss: 0.4419 - val_dice_value: 0.9941\n",
      "Epoch 9/50\n",
      "255/255 [==============================] - 152s - loss: 0.4519 - dice_value: 0.9925 - val_loss: 0.4432 - val_dice_value: 0.9937\n",
      "Epoch 10/50\n",
      "255/255 [==============================] - 152s - loss: 0.4517 - dice_value: 0.9925 - val_loss: 0.4417 - val_dice_value: 0.9941\n",
      "Epoch 11/50\n",
      "255/255 [==============================] - 153s - loss: 0.4515 - dice_value: 0.9924 - val_loss: 0.4410 - val_dice_value: 0.9944\n",
      "Epoch 12/50\n",
      "255/255 [==============================] - 152s - loss: 0.4506 - dice_value: 0.9928 - val_loss: 0.4403 - val_dice_value: 0.9946\n",
      "Epoch 13/50\n",
      "255/255 [==============================] - 152s - loss: 0.4504 - dice_value: 0.9930 - val_loss: 0.4415 - val_dice_value: 0.9942\n",
      "Epoch 14/50\n",
      "255/255 [==============================] - 153s - loss: 0.4505 - dice_value: 0.9929 - val_loss: 0.4399 - val_dice_value: 0.9946\n",
      "Epoch 15/50\n",
      "255/255 [==============================] - 153s - loss: 0.4497 - dice_value: 0.9932 - val_loss: 0.4405 - val_dice_value: 0.9945\n",
      "Epoch 16/50\n",
      "255/255 [==============================] - 152s - loss: 0.4502 - dice_value: 0.9930 - val_loss: 0.4402 - val_dice_value: 0.9945\n",
      "Epoch 17/50\n",
      "255/255 [==============================] - 153s - loss: 0.4490 - dice_value: 0.9932 - val_loss: 0.4392 - val_dice_value: 0.9949\n",
      "Epoch 18/50\n",
      "255/255 [==============================] - 153s - loss: 0.4486 - dice_value: 0.9934 - val_loss: 0.4388 - val_dice_value: 0.9950\n",
      "Epoch 19/50\n",
      "255/255 [==============================] - 152s - loss: 0.4481 - dice_value: 0.9935 - val_loss: 0.4395 - val_dice_value: 0.9947\n",
      "Epoch 20/50\n",
      "255/255 [==============================] - 152s - loss: 0.4498 - dice_value: 0.9926 - val_loss: 0.4882 - val_dice_value: 0.9582\n",
      "Epoch 21/50\n",
      "255/255 [==============================] - 152s - loss: 0.4494 - dice_value: 0.9932 - val_loss: 0.4389 - val_dice_value: 0.9950\n",
      "Epoch 22/50\n",
      "255/255 [==============================] - 153s - loss: 0.4487 - dice_value: 0.9935 - val_loss: 0.4383 - val_dice_value: 0.9951\n",
      "Epoch 23/50\n",
      "255/255 [==============================] - 152s - loss: 0.4480 - dice_value: 0.9935 - val_loss: 0.4385 - val_dice_value: 0.9950\n",
      "Epoch 24/50\n",
      "255/255 [==============================] - 153s - loss: 0.4480 - dice_value: 0.9936 - val_loss: 0.4378 - val_dice_value: 0.9952\n",
      "Epoch 25/50\n",
      "255/255 [==============================] - 153s - loss: 0.4465 - dice_value: 0.9938 - val_loss: 0.4379 - val_dice_value: 0.9952\n",
      "Epoch 26/50\n",
      "255/255 [==============================] - 159s - loss: 0.4474 - dice_value: 0.9937 - val_loss: 0.4379 - val_dice_value: 0.9952\n",
      "Epoch 27/50\n",
      "255/255 [==============================] - 154s - loss: 0.4468 - dice_value: 0.9938 - val_loss: 0.4376 - val_dice_value: 0.9953\n",
      "Epoch 28/50\n",
      "255/255 [==============================] - 155s - loss: 0.4476 - dice_value: 0.9937 - val_loss: 0.4388 - val_dice_value: 0.9949\n",
      "Epoch 29/50\n",
      "254/255 [============================>.] - ETA: 0s - loss: 0.4466 - dice_value: 0.9938\n",
      "Epoch 00028: reducing learning rate to 0.00010000000475.\n",
      "255/255 [==============================] - 154s - loss: 0.4466 - dice_value: 0.9938 - val_loss: 0.4382 - val_dice_value: 0.9951\n",
      "Epoch 30/50\n",
      "255/255 [==============================] - 156s - loss: 0.4458 - dice_value: 0.9941 - val_loss: 0.4365 - val_dice_value: 0.9955\n",
      "Epoch 31/50\n",
      "255/255 [==============================] - 158s - loss: 0.4456 - dice_value: 0.9943 - val_loss: 0.4363 - val_dice_value: 0.9956\n",
      "Epoch 32/50\n",
      "255/255 [==============================] - 158s - loss: 0.4449 - dice_value: 0.9943 - val_loss: 0.4361 - val_dice_value: 0.9956\n",
      "Epoch 33/50\n",
      "255/255 [==============================] - 155s - loss: 0.4452 - dice_value: 0.9943 - val_loss: 0.4360 - val_dice_value: 0.9957\n",
      "Epoch 34/50\n",
      "255/255 [==============================] - 159s - loss: 0.4445 - dice_value: 0.9944 - val_loss: 0.4360 - val_dice_value: 0.9957\n",
      "Epoch 35/50\n",
      "255/255 [==============================] - 159s - loss: 0.4440 - dice_value: 0.9945 - val_loss: 0.4359 - val_dice_value: 0.9957\n",
      "Epoch 36/50\n",
      "255/255 [==============================] - 162s - loss: 0.4446 - dice_value: 0.9945 - val_loss: 0.4359 - val_dice_value: 0.9957\n",
      "Epoch 37/50\n",
      "245/255 [===========================>..] - ETA: 5s - loss: 0.4442 - dice_value: 0.9945"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "batch_size = 16\n",
    "run_name = utils.get_run_name('weights/', 'unet')\n",
    "weight_path = 'weights/{}.hdf5'.format(run_name)\n",
    "\n",
    "callbacks = [EarlyStopping(monitor='val_dice_value',\n",
    "                           patience=8,\n",
    "                           verbose=1,\n",
    "                           min_delta=1e-4,\n",
    "                           mode='max'),\n",
    "             ReduceLROnPlateau(monitor='val_dice_value',\n",
    "                               factor=0.1,\n",
    "                               patience=4,\n",
    "                               verbose=1,\n",
    "                               epsilon=1e-4,\n",
    "                               mode='max'),\n",
    "             ModelCheckpoint(monitor='val_dice_value',\n",
    "                             filepath=weight_path,\n",
    "                             save_best_only=True,\n",
    "                             save_weights_only=True,\n",
    "                             mode='max'),\n",
    "             TensorBoard(log_dir='logs/{}'.format(run_name), batch_size=batch_size)]\n",
    "\n",
    "#model.load_weights('weights/best_weights.hdf5')\n",
    "#K.set_value(model.optimizer.lr, 0.01)\n",
    "\n",
    "model.fit_generator(generator=train_generator(batch_size),\n",
    "                    steps_per_epoch=np.ceil(float(len(ids_train_split)) / float(batch_size)),\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    callbacks=callbacks,\n",
    "                    validation_data=valid_generator(batch_size),\n",
    "                    validation_steps=np.ceil(float(len(ids_valid_split)) / float(batch_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def np_dice_value(y_true, y_pred):\n",
    "    smooth = 1.\n",
    "    y_true_f = y_true.flatten()\n",
    "    y_pred_f = y_pred.flatten()\n",
    "    intersection = np.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (np.sum(y_true_f) + np.sum(y_pred_f) + smooth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "run_name = 'unet-2017-08-20-5'\n",
    "model.load_weights('weights/{}.hdf5'.format(run_name))\n",
    "\n",
    "val_imgs, val_masks = next(valid_generator(len(ids_valid_split)))\n",
    "val_imgs = np.array(val_imgs)\n",
    "val_masks = np.array(val_masks)\n",
    "val_pred_masks = model.predict(val_imgs, batch_size=32)\n",
    "masks_val_dices = [np_dice_value(mask, pred_mask) for (mask, pred_mask) in zip(val_masks, val_pred_masks)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "hist, bins = np.histogram(masks_val_dices, bins=50)\n",
    "width = 0.7 * (bins[1] - bins[0])\n",
    "center = (bins[:-1] + bins[1:]) / 2\n",
    "plt.bar(center, hist, align='center', width=width)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "index = 10\n",
    "img_path = train_path.format(ids_valid_split.values[index])\n",
    "utils.show_mask(img_path, \n",
    "                val_masks[index].squeeze(axis=-1),\n",
    "                val_pred_masks[index].squeeze(axis=-1),\n",
    "                show_img=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Create model first if required\n",
    "run_name = 'unet-2017-08-20-5'\n",
    "model.load_weights('weights/{}.hdf5'.format(run_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Generate Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "orig_size = (1918, 1280)\n",
    "batch_size = 2\n",
    "threshold = 0.5\n",
    "generate_submit(input_size, orig_size, batch_size, threshold, model, 'inputs/test/', 'submits/', run_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras-tf",
   "language": "python",
   "name": "keras-tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
