{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carvana Unet Pseudo Labeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TensorBoard\n",
    "import keras.backend as K\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from optimizers.AdamAccumulate import AdamAccumulate\n",
    "#from models.u_net import UNet\n",
    "#from models.u_net_aux import UNet_Aux\n",
    "from models.u_net_heng import UNet_Heng\n",
    "from utilities.submit import generate_submit\n",
    "from utilities import utils_masks as utils\n",
    "from utilities.losses import weighted_bce_dice_loss, dice_value\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group 16|01|02:   #Training = 758   #Validation = 196   #Test = 10000\n"
     ]
    }
   ],
   "source": [
    "utils.set_results_reproducible()\n",
    "\n",
    "input_size = 128\n",
    "num_pseud_data = 10000\n",
    "\n",
    "train_path = \"inputs/train/{}.jpg\" \n",
    "train_mask_path = \"inputs/train_masks/{}_mask.gif\"\n",
    "\n",
    "test_path = \"inputs/test/{}.jpg\"\n",
    "test_mask_path = \"inputs/test_masks/{}_mask.gif\"\n",
    "\n",
    "#bboxes = None\n",
    "bbox_file_path = 'inputs/data_bbox.csv'\n",
    "bboxes = utils.get_bboxes(bbox_file_path)\n",
    "\n",
    "train_df = pd.read_csv('inputs/train_masks.csv')\n",
    "all_ids_train = train_df['img'].map(lambda s: s.split('.')[0])\n",
    "all_ids_train_split, all_ids_valid_split = train_test_split(all_ids_train, test_size=0.2, random_state=42)\n",
    "\n",
    "test_df = pd.read_csv('inputs/sample_submission.csv')\n",
    "\n",
    "groups = [['16', '01', '02']]#,['08', '09', '10'],'04|05|06','12|13|14','15|03','07|11']\n",
    "ids_train_splits = {}\n",
    "ids_valid_splits = {}\n",
    "ids_test_splits = {}\n",
    "test_df_group = pd.DataFrame([])\n",
    "\n",
    "\n",
    "for group in groups:\n",
    "    group_name = group[0]+'|'+group[1]+'|'+group[2]\n",
    "    df_group = train_df[(train_df.img.str.match('^.*_(' + group_name + ').jpg$'))]\n",
    "    ids_group = df_group['img'].map(lambda s: s.split('.')[0])\n",
    "    ids_train_split = pd.Series(list(set(all_ids_train_split).intersection(set(ids_group))))\n",
    "    ids_valid_split = pd.Series(list(set(all_ids_valid_split).intersection(set(ids_group))))\n",
    "    ids_train_splits[group_name] = ids_train_split\n",
    "    ids_valid_splits[group_name] = ids_valid_split\n",
    "    \n",
    "    for num in range(len(group)):\n",
    "        test_df_group = pd.concat([test_df_group, (test_df[(test_df.img.str.match('^.*_(' + group[num] + ').jpg$'))].sample(n=num_pseud_data/3))])\n",
    "     \n",
    "    test_df_group = test_df_group.sample(n=num_pseud_data, replace=True)\n",
    "    ids_test_splits[group_name] = test_df_group['img'].map(lambda s: s.split('.')[0])\n",
    "    print('group {0}:   #Training = {1}   #Validation = {2}   #Test = {3}'.format(group_name, \n",
    "                                                                    len(ids_train_split), \n",
    "                                                                    len(ids_valid_split),\n",
    "                                                                    len(ids_test_splits[group_name])))\n",
    "\n",
    "def train_generator(batch_size, group, outputs=None):\n",
    "    return utils.train_generator(train_path, train_mask_path, ids_train_splits[group], \n",
    "                                 input_size, batch_size, bboxes, outputs=outputs,\n",
    "                                 augmentations=['HUE_SATURATION', 'SHIFT_SCALE'])\n",
    "\n",
    "def valid_generator(batch_size, group, outputs=None):\n",
    "    return utils.valid_generator(train_path, train_mask_path, ids_valid_splits[group],\n",
    "                                 input_size, batch_size, bboxes, outputs=outputs)\n",
    "\n",
    "\n",
    "def pseudo_generator(batch_size, group, accum_iters, outputs=None):\n",
    "    ids = utils.make_list_ids(ids_train_splits[group], ids_test_splits[group_name], batch_size, accum_iters)\n",
    "    return utils.pseudo_generator(train_path, train_mask_path, test_path, test_mask_path, ids,\n",
    "                                 input_size, batch_size, bboxes, outputs=outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = next(pseudo_generator(16, '16|01|02', 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print np.shape(x[0])\n",
    "print np.shape(x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx = 8\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(np.squeeze(x[1][idx]))\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(np.squeeze(x[0][idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#U-Net-Aux:\n",
    "#model = UNet_Aux((input_size, input_size, 3), filters=64, depth=4, dropout_base_only=False, dropout=0,\n",
    "#                 activation=lambda x: PReLU()(x), init='he_uniform', auxiliaries=[False, True, True, False])\n",
    "#outputs = {'aux_out1':2**-1, 'aux_out2':2**-2, 'main_out':1}\n",
    "#weights = {'aux_out1':0.2, 'aux_out2':0.05, 'main_out':1.}\n",
    "#model.compile(optimizer=AdamAccumulate(accum_iters=4), \n",
    "#              loss=weighted_bce_dice_loss, metrics=[dice_value], loss_weights=weights)\n",
    "\n",
    "#U-Net:\n",
    "#model = UNet((input_size, input_size, 3), filters=64, depth=4, dropout_base_only=False, dropout=0,\n",
    "#             activation=lambda x: PReLU()(x), init='he_uniform')\n",
    "#model.compile(optimizer=AdamAccumulate(accum_iters=4), loss=weighted_bce_dice_loss, metrics=[dice_value])\n",
    "\n",
    "models = {}\n",
    "for group in groups:\n",
    "    group_name = group[0]+'|'+group[1]+'|'+group[2]\n",
    "    model = UNet_Heng((input_size, input_size, 3))\n",
    "    models[group_name] = model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting run \"unet-heng-16|01|02-2017-09-25-0753\"\n",
      "Epoch 1/150\n",
      "673/673 [==============================] - 328s - loss: 0.4567 - dice_value: 0.9926 - val_loss: 0.4444 - val_dice_value: 0.9950\n",
      "Epoch 2/150\n",
      "673/673 [==============================] - 321s - loss: 0.4552 - dice_value: 0.9930 - val_loss: 0.4462 - val_dice_value: 0.9944\n",
      "Epoch 3/150\n",
      "673/673 [==============================] - 322s - loss: 0.4545 - dice_value: 0.9934 - val_loss: 0.4431 - val_dice_value: 0.9953\n",
      "Epoch 4/150\n",
      "673/673 [==============================] - 327s - loss: 0.4540 - dice_value: 0.9933 - val_loss: 0.4426 - val_dice_value: 0.9954\n",
      "Epoch 5/150\n",
      "673/673 [==============================] - 325s - loss: 0.4530 - dice_value: 0.9937 - val_loss: 0.4426 - val_dice_value: 0.9954\n",
      "Epoch 6/150\n",
      "673/673 [==============================] - 325s - loss: 0.4528 - dice_value: 0.9937 - val_loss: 0.4422 - val_dice_value: 0.9955\n",
      "Epoch 7/150\n",
      "673/673 [==============================] - 325s - loss: 0.4533 - dice_value: 0.9938 - val_loss: 0.4429 - val_dice_value: 0.9953\n",
      "Epoch 8/150\n",
      "673/673 [==============================] - 324s - loss: 0.4526 - dice_value: 0.9937 - val_loss: 0.4418 - val_dice_value: 0.9956\n",
      "Epoch 9/150\n",
      "673/673 [==============================] - 326s - loss: 0.4517 - dice_value: 0.9940 - val_loss: 0.4417 - val_dice_value: 0.9956\n",
      "Epoch 10/150\n",
      "673/673 [==============================] - 324s - loss: 0.4527 - dice_value: 0.9938 - val_loss: 0.4420 - val_dice_value: 0.9955\n",
      "Epoch 11/150\n",
      "673/673 [==============================] - 325s - loss: 0.4521 - dice_value: 0.9939 - val_loss: 0.4416 - val_dice_value: 0.9956\n",
      "Epoch 12/150\n",
      "673/673 [==============================] - 324s - loss: 0.4516 - dice_value: 0.9941 - val_loss: 0.4412 - val_dice_value: 0.9957\n",
      "Epoch 13/150\n",
      "673/673 [==============================] - 325s - loss: 0.4517 - dice_value: 0.9940 - val_loss: 0.4416 - val_dice_value: 0.9956\n",
      "Epoch 14/150\n",
      "673/673 [==============================] - 324s - loss: 0.4514 - dice_value: 0.9941 - val_loss: 0.4415 - val_dice_value: 0.9957\n",
      "Epoch 15/150\n",
      "673/673 [==============================] - 324s - loss: 0.4510 - dice_value: 0.9941 - val_loss: 0.4417 - val_dice_value: 0.9956\n",
      "Epoch 16/150\n",
      "673/673 [==============================] - 325s - loss: 0.4508 - dice_value: 0.9943 - val_loss: 0.4417 - val_dice_value: 0.9956\n",
      "Epoch 17/150\n",
      "672/673 [============================>.] - ETA: 0s - loss: 0.4512 - dice_value: 0.9942\n",
      "Epoch 00016: reducing learning rate to 0.00010000000475.\n",
      "673/673 [==============================] - 325s - loss: 0.4513 - dice_value: 0.9942 - val_loss: 0.4418 - val_dice_value: 0.9956\n",
      "Epoch 18/150\n",
      "673/673 [==============================] - 325s - loss: 0.4504 - dice_value: 0.9945 - val_loss: 0.4403 - val_dice_value: 0.9960\n",
      "Epoch 19/150\n",
      "673/673 [==============================] - 326s - loss: 0.4498 - dice_value: 0.9946 - val_loss: 0.4402 - val_dice_value: 0.9960\n",
      "Epoch 20/150\n",
      "673/673 [==============================] - 325s - loss: 0.4490 - dice_value: 0.9947 - val_loss: 0.4402 - val_dice_value: 0.9960\n",
      "Epoch 21/150\n",
      "673/673 [==============================] - 324s - loss: 0.4493 - dice_value: 0.9947 - val_loss: 0.4402 - val_dice_value: 0.9960\n",
      "Epoch 22/150\n",
      "673/673 [==============================] - 326s - loss: 0.4499 - dice_value: 0.9946 - val_loss: 0.4401 - val_dice_value: 0.9960\n",
      "Epoch 23/150\n",
      "672/673 [============================>.] - ETA: 0s - loss: 0.4488 - dice_value: 0.9948\n",
      "Epoch 00022: reducing learning rate to 1.0000000475e-05.\n",
      "673/673 [==============================] - 327s - loss: 0.4488 - dice_value: 0.9948 - val_loss: 0.4402 - val_dice_value: 0.9960\n",
      "Epoch 24/150\n",
      "673/673 [==============================] - 325s - loss: 0.4493 - dice_value: 0.9948 - val_loss: 0.4401 - val_dice_value: 0.9960\n",
      "Epoch 25/150\n",
      "673/673 [==============================] - 325s - loss: 0.4490 - dice_value: 0.9948 - val_loss: 0.4400 - val_dice_value: 0.9960\n",
      "Epoch 26/150\n",
      "673/673 [==============================] - 325s - loss: 0.4488 - dice_value: 0.9948 - val_loss: 0.4401 - val_dice_value: 0.9960\n",
      "Epoch 27/150\n",
      "672/673 [============================>.] - ETA: 0s - loss: 0.4489 - dice_value: 0.9948\n",
      "Epoch 00026: reducing learning rate to 1.00000006569e-06.\n",
      "673/673 [==============================] - 324s - loss: 0.4489 - dice_value: 0.9948 - val_loss: 0.4401 - val_dice_value: 0.9960\n",
      "Epoch 00026: early stopping\n"
     ]
    }
   ],
   "source": [
    "epochs = 150\n",
    "batch_size = 16\n",
    "accum_iters = 4\n",
    "\n",
    "for group in groups:\n",
    "    group_name = group[0]+'|'+group[1]+'|'+group[2]\n",
    "    model = models[group_name]\n",
    "    model.compile(optimizer=AdamAccumulate(accum_iters=accum_iters),\n",
    "                  loss=weighted_bce_dice_loss, metrics=[dice_value])\n",
    "    run_name = utils.get_run_name('weights/{}.hdf5', 'unet-heng-{}'.format(group_name))\n",
    "    weights_path = 'weights/{}.hdf5'.format(run_name)\n",
    "\n",
    "    callbacks = [EarlyStopping(monitor='val_dice_value',\n",
    "                               patience=8,\n",
    "                               verbose=1,\n",
    "                               min_delta=1e-4,\n",
    "                               mode='max'),\n",
    "                 ReduceLROnPlateau(monitor='val_dice_value',\n",
    "                                   factor=0.1,\n",
    "                                   patience=4,\n",
    "                                   verbose=1,\n",
    "                                   epsilon=1e-4,\n",
    "                                   mode='max'),\n",
    "                 ModelCheckpoint(monitor='val_dice_value',\n",
    "                                 filepath=weights_path,\n",
    "                                 save_best_only=True,\n",
    "                                 save_weights_only=True,\n",
    "                                 mode='max'),\n",
    "                 TensorBoard(log_dir='logs/{}'.format(run_name), batch_size=batch_size)]\n",
    "\n",
    "    model.load_weights('weights/unet-heng-2017-09-23-1907.hdf5')\n",
    "    #K.set_value(model.optimizer.lr, 1e-4)\n",
    "\n",
    "    num_train_data = len(ids_train_splits[group_name])+len(ids_test_splits[group_name])\n",
    "    print('Starting run \"{}\"'.format(run_name))\n",
    "    model.fit_generator(generator=pseudo_generator(batch_size, group_name, accum_iters),\n",
    "                        steps_per_epoch=np.ceil(float(num_train_data)/float(batch_size)),\n",
    "                        epochs=epochs,\n",
    "                        verbose=1,\n",
    "                        callbacks=callbacks,\n",
    "                        validation_data=valid_generator(batch_size, group_name),\n",
    "                        validation_steps=np.ceil(float(len(ids_valid_splits[group_name])) / float(batch_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Pseudo Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from shutil import copy\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copy test samples in aspecific direction to a new folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "src = 'inputs/test_hq/{}'\n",
    "des = 'inputs/test_hq_06_13'\n",
    "\n",
    "df = pd.read_csv('inputs/sample_submission.csv')\n",
    "\n",
    "groups = ['06|13']#,'08|09|10','04|05|06','12|13|14','15|03','07|11']\n",
    "for group in groups:\n",
    "    df_group = df[(df.img.str.match('^.*_(' + group + ').jpg$'))]['img']\n",
    "    if not os.path.exists(des):\n",
    "        os.makedirs(des) \n",
    "    for im_name in df_group:\n",
    "        copy(src.format(im_name), des)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict related masks of "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "src = 'inputs/test_{}/'\n",
    "des = 'inputs/test_masks_{}/'\n",
    "\n",
    "#bboxes = None\n",
    "bbox_file_path = 'inputs/test_bbox.csv'\n",
    "bboxes = utils.get_bboxes(bbox_file_path)\n",
    "\n",
    "groups = [['16', '01', '02']]#,['08', '09', '10'],'04|05|06','12|13|14','15|03','07|11']\n",
    "for group in groups:\n",
    "    group_name = group[0]+'|'+group[1]+'|'+group[2]\n",
    "    model = models[group_name]\n",
    "    model.load_weights('weights/unet-2017-09-03-1739.hdf5')\n",
    "    if not os.path.exists(des.format(group_name)):\n",
    "        os.makedirs(des.format(group_name))\n",
    "    generate_submit(model, input_size, batch_size=16, threshold=0.5, test_path=src.format(group_name), \n",
    "                    submit_path='outputs/',\n",
    "                    run_name='generate_test_masks_1', test_masks_path=des.format(group_name), bboxes=bboxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def np_dice_value(y_true, y_pred):\n",
    "    smooth = 1.\n",
    "    y_true_f = y_true.flatten()\n",
    "    y_pred_f = y_pred.flatten()\n",
    "    intersection = np.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (np.sum(y_true_f) + np.sum(y_pred_f) + smooth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "run_name = 'unet-2017-09-03-1739'\n",
    "model.load_weights('weights/{}.hdf5'.format(run_name))\n",
    "\n",
    "val_imgs, val_masks = next(valid_generator(len(ids_valid_split)))\n",
    "val_imgs = np.array(val_imgs)\n",
    "val_masks = np.array(val_masks)\n",
    "val_pred_masks = model.predict(val_imgs, batch_size=1)\n",
    "masks_val_dices = [np_dice_value(mask, pred_mask) for (mask, pred_mask) in zip(val_masks, val_pred_masks)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Display the worst predicted mask for validation examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "index = np.argsort(masks_val_dices)[7]\n",
    "id = ids_valid_split.values[index]\n",
    "utils.show_mask(train_path.format(id), val_masks[index].squeeze(), val_pred_masks[index].squeeze(), show_img=False)\n",
    "print id, masks_val_dices[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "indices = np.argsort(masks_val_dices[masks_val_dices <= 99.6])\n",
    "for id in indices:\n",
    "        print(masks_val_dices[id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "hist, bins = np.histogram(masks_val_dices, bins=50)\n",
    "width = 0.7 * (bins[1] - bins[0])\n",
    "center = (bins[:-1] + bins[1:]) / 2\n",
    "plt.bar(center, hist, align='center', width=width)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "indices = np.random.randint(len(ids_valid_split), size=3)\n",
    "for index in indices:\n",
    "    id = ids_valid_split.values[index]\n",
    "    utils.show_mask(train_path.format(id), val_masks[index].squeeze(), val_pred_masks[index].squeeze(),\n",
    "                    show_img=True, bbox = bboxes[id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Create model first if required\n",
    "run_name = 'unet-2017-08-20-5'\n",
    "model.load_weights('weights/{}.hdf5'.format(run_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Generate Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "threshold = 0.5\n",
    "test_path = 'inputs/test1/' #'inputs/test/'\n",
    "test_masks_path = 'outputs/test1_masks/' #None\n",
    "generate_submit(model, input_size, batch_size, threshold, test_path, 'outputs/', run_name, test_masks_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "utils.show_test_masks(test_path, test_masks_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
