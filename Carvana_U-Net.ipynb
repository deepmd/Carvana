{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carvana U-Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TensorBoard\n",
    "from keras.layers import AveragePooling2D\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from models.u_net import UNet\n",
    "import utils\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 128\n",
    "train_path = \"input/train/{}.jpg\" \n",
    "train_mask_path = \"input/train_masks/{}_mask.gif\"\n",
    "df_train = pd.read_csv('input/train_masks.csv')\n",
    "ids_train = df_train['img'].map(lambda s: s.split('.')[0])#[:3000]\n",
    "ids_train_split, ids_valid_split = train_test_split(ids_train, test_size=0.2, random_state=42)\n",
    "\n",
    "print('Training on {} samples'.format(len(ids_train_split)))\n",
    "print('Validating on {} samples'.format(len(ids_valid_split)))\n",
    "\n",
    "def train_generator(batch_size):\n",
    "    return utils.train_generator(train_path, train_mask_path, ids_train_split, input_size, batch_size)\n",
    "\n",
    "def valid_generator(batch_size):\n",
    "    return utils.valid_generator(train_path, train_mask_path, ids_valid_split, input_size, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_value(y_true, y_pred):\n",
    "    smooth = 1.\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "#def bce_dice_loss(y_true, y_pred):\n",
    "#    return binary_crossentropy(y_true, y_pred) + (1 - dice_value(y_true, y_pred))\n",
    "\n",
    "def weighted_bce_loss(y_true, y_pred, weights):\n",
    "    return K.mean(tf.nn.weighted_cross_entropy_with_logits(y_true, y_pred, weights), axis=-1)\n",
    "\n",
    "def weighted_dice_value(y_true, y_pred, weights):\n",
    "    smooth = 1.\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = y_true_f * y_pred_f\n",
    "    weights = K.flatten(weights)\n",
    "    w2 = weights * weights\n",
    "    return (2. * K.sum(w2 * intersection) + smooth) / (K.sum(w2 * y_true_f) + K.sum(w2 * y_pred_f) + smooth)\n",
    "\n",
    "def weighted_bce_dice_loss(y_true, y_pred):\n",
    "    a = AveragePooling2D(pool_size=(11, 11), strides=1, padding='same')(y_true)\n",
    "    ind = K.cast(K.greater_equal(a, 0.01), 'float32') * K.cast(K.less_equal(a, 0.99), 'float32')\n",
    "    ind = K.cast(ind, 'float32')\n",
    "    weights = K.ones_like(a)\n",
    "    w0 = K.sum(weights)\n",
    "    weights = weights + ind*2\n",
    "    w1 = K.sum(weights)\n",
    "    weights = weights/w1*w0\n",
    "    return  weighted_bce_loss(y_true, y_pred, weights) + (1 - weighted_dice_value(y_true, y_pred, weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet((input_size, input_size, 3))\n",
    "model.compile(optimizer=Adam(1e-3), loss=weighted_bce_dice_loss, metrics=[dice_value])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_size = 16\n",
    "run_number = 2\n",
    "weight_path = \"weights/UNet-\" + str(run_number) + \"-{epoch:02d}-{dice_value:.4f}-{val_dice_value:.4f}.hdf5\"\n",
    "\n",
    "callbacks = [EarlyStopping(monitor='val_dice_value',\n",
    "                           patience=8,\n",
    "                           verbose=1,\n",
    "                           min_delta=1e-4,\n",
    "                           mode='max'),\n",
    "             ReduceLROnPlateau(monitor='val_dice_value',\n",
    "                               factor=0.1,\n",
    "                               patience=4,\n",
    "                               verbose=1,\n",
    "                               epsilon=1e-4,\n",
    "                               mode='max'),\n",
    "             ModelCheckpoint(monitor='val_dice_value',\n",
    "                             filepath=weight_path,\n",
    "                             save_best_only=True,\n",
    "                             save_weights_only=True,\n",
    "                             mode='max'),\n",
    "             TensorBoard(log_dir='logs/U{:d}'.format(run_number), batch_size=batch_size)]\n",
    "\n",
    "#model.load_weights('weights/best_weights.hdf5')\n",
    "#K.set_value(model.optimizer.lr, 0.01)\n",
    "\n",
    "model.fit_generator(generator=train_generator(batch_size),\n",
    "                    steps_per_epoch=np.ceil(float(len(ids_train_split)) / float(batch_size)),\n",
    "                    epochs=epochs,\n",
    "                    verbose=2,\n",
    "                    callbacks=callbacks,\n",
    "                    validation_data=valid_generator(batch_size),\n",
    "                    validation_steps=np.ceil(float(len(ids_valid_split)) / float(batch_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def np_dice_value(y_true, y_pred):\n",
    "    smooth = 1.\n",
    "    y_true_f = y_true.flatten()\n",
    "    y_pred_f = y_pred.flatten()\n",
    "    intersection = np.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (np.sum(y_true_f) + np.sum(y_pred_f) + smooth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"weights/UNet-2-09-0.9880-0.9923.hdf5\")\n",
    "\n",
    "val_imgs, val_masks = next(valid_generator(len(ids_valid_split)))\n",
    "val_imgs = np.array(val_imgs)\n",
    "val_masks = np.array(val_masks)\n",
    "val_pred_masks = model.predict(val_imgs, batch_size=32)\n",
    "masks_val_dices = [np_dice_value(mask, pred_mask) for (mask, pred_mask) in zip(val_masks, val_pred_masks)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist, bins = np.histogram(masks_val_dices, bins=50)\n",
    "width = 0.7 * (bins[1] - bins[0])\n",
    "center = (bins[:-1] + bins[1:]) / 2\n",
    "plt.bar(center, hist, align='center', width=width)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "index = 10\n",
    "img_path = train_path.format(ids_valid_split.values[index])\n",
    "utils.show_mask(input_size, \n",
    "                val_masks[index].squeeze(axis=-1),\n",
    "                val_pred_masks[index].squeeze(axis=-1),\n",
    "                img_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
