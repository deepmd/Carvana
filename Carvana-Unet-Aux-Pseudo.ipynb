{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carvana U-Net Angles+Auxiliary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TensorBoard\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from optimizers.AdamAccumulate import AdamAccumulate\n",
    "from models.u_net_heng_aux import UNet_Heng_Aux\n",
    "from utilities.submit import generate_submit\n",
    "from utilities import utils_masks as utils\n",
    "from utilities.losses import weighted_bce_dice_loss, dice_value\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Training = 4070+6000   #Validation = 1018\n"
     ]
    }
   ],
   "source": [
    "utils.set_results_reproducible()\n",
    "input_size = 1024\n",
    "num_pseudo_data = 6000\n",
    "train_path = \"inputs/train_hq/{}.jpg\" \n",
    "train_mask_path = \"inputs/train_masks/{}_mask.gif\"\n",
    "train_df = pd.read_csv('inputs/train_masks.csv')\n",
    "test_path = \"inputs/test_hq/{}.jpg\"\n",
    "test_mask_path = \"outputs/test_hq_masks/{}_mask.gif\"\n",
    "test_df = pd.read_csv('inputs/sample_submission.csv')\n",
    "\n",
    "ids_train = train_df['img'].map(lambda s: s.split('.')[0])\n",
    "ids_train_split, ids_valid_split = train_test_split(ids_train, test_size=0.2, random_state=42)\n",
    "\n",
    "test_df = test_df.sample(n=num_pseudo_data)\n",
    "ids_test_split = test_df['img'].map(lambda s: s.split('.')[0])\n",
    "\n",
    "print('#Training = {0}+{1}   #Validation = {2}'\n",
    "      .format(len(ids_train_split), len(ids_test_split), len(ids_valid_split)))\n",
    "\n",
    "bbox_file_path = 'inputs/data_bbox.csv'\n",
    "bboxes = utils.get_bboxes(bbox_file_path)\n",
    "\n",
    "def get_pseudo_ids(batch_size, accum_iters):\n",
    "    return utils.make_list_ids(ids_train_split, ids_test_split, batch_size, accum_iters)\n",
    "\n",
    "def valid_generator(batch_size, outputs=None):\n",
    "    return utils.valid_generator(train_path, train_mask_path, ids_valid_split,\n",
    "                                 input_size, batch_size, bboxes, outputs=outputs)\n",
    "\n",
    "def pseudo_generator(batch_size, pseudo_ids, outputs=None):\n",
    "    return utils.pseudo_generator(train_path, train_mask_path, test_path, test_mask_path, pseudo_ids,\n",
    "                                  input_size, batch_size, bboxes, outputs=outputs,\n",
    "                                  augmentations=['HUE_SATURATION', 'SHIFT_SCALE', 'FLIP'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "source": [
    "def copy_weights(source, target):\n",
    "    source_layers = [layer for layer in source.layers if not layer.name.startswith('aux_out')]\n",
    "    for s_layer,t_layer in zip(source_layers, target.layers):\n",
    "        print('{0} --> {1}'.format(s_layer.name, t_layer.name))\n",
    "        t_layer.set_weights(s_layer.get_weights())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "source": [
    "#U-Net-Aux:\n",
    "#model = UNet_Aux((input_size, input_size, 3), filters=64, depth=4, dropout_base_only=False, dropout=0,\n",
    "#                 activation=lambda x: PReLU()(x), init='he_uniform', auxiliaries=[False, True, True, False])\n",
    "#outputs = {'aux_out1':2**-1, 'aux_out2':2**-2, 'main_out':1}\n",
    "#weights = {'aux_out1':0.2, 'aux_out2':0.05, 'main_out':1.}\n",
    "#model.compile(optimizer=AdamAccumulate(accum_iters=4), \n",
    "#              loss=weighted_bce_dice_loss, metrics=[dice_value], loss_weights=weights)\n",
    "\n",
    "#U-Net:\n",
    "auxiliaries = [False, False, False, False, True, True]\n",
    "model_aux = UNet_Heng_Aux((input_size, input_size, 3), auxiliaries=auxiliaries)\n",
    "model_aux.load_weights('weights/unet-heng-aux-2017-09-25-1603.hdf5')\n",
    "model = UNet_Heng((input_size, input_size, 3))\n",
    "copy_weights(model_aux, model)\n",
    "model.save_weights('weights/unet-heng-2017-09-25-1603.hdf5')\n",
    "\n",
    "#U-Net-Heng:\n",
    "#model = UNet_Heng((input_size, input_size, 3))\n",
    "#model.load_weights('weights/unet-heng-crop-2017-09-18-1053.hdf5')\n",
    "#model_aux = UNet_Heng_Aux((input_size, input_size, 3),\n",
    "#                          auxiliaries=[False, False, False, False, True, True])\n",
    "#copy_weights(model, model_aux)\n",
    "#model_aux.save_weights('weights/unet-heng-aux-base.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting run \"unet-ultimate-2017-09-26-2148\"\n",
      "Epoch 1/150\n",
      "24032/24032 [==============================] - 13383s - loss: 0.5989 - aux_out4_loss: 0.4625 - aux_out5_loss: 0.4687 - main_out_loss: 0.4830 - aux_out4_dice_value: 0.9952 - aux_out5_dice_value: 0.9930 - main_out_dice_value: 0.9969 - val_loss: 0.5961 - val_aux_out4_loss: 0.4574 - val_aux_out5_loss: 0.4639 - val_main_out_loss: 0.4815 - val_aux_out4_dice_value: 0.9954 - val_aux_out5_dice_value: 0.9938 - val_main_out_dice_value: 0.9968\n",
      "Epoch 2/150\n",
      "24032/24032 [==============================] - 13379s - loss: 0.5985 - aux_out4_loss: 0.4617 - aux_out5_loss: 0.4670 - main_out_loss: 0.4828 - aux_out4_dice_value: 0.9955 - aux_out5_dice_value: 0.9940 - main_out_dice_value: 0.9970 - val_loss: 0.5959 - val_aux_out4_loss: 0.4571 - val_aux_out5_loss: 0.4630 - val_main_out_loss: 0.4813 - val_aux_out4_dice_value: 0.9955 - val_aux_out5_dice_value: 0.9943 - val_main_out_dice_value: 0.9968\n",
      "Epoch 3/150\n",
      "24032/24032 [==============================] - 13383s - loss: 0.5981 - aux_out4_loss: 0.4612 - aux_out5_loss: 0.4661 - main_out_loss: 0.4825 - aux_out4_dice_value: 0.9957 - aux_out5_dice_value: 0.9945 - main_out_dice_value: 0.9971 - val_loss: 0.5957 - val_aux_out4_loss: 0.4567 - val_aux_out5_loss: 0.4623 - val_main_out_loss: 0.4812 - val_aux_out4_dice_value: 0.9956 - val_aux_out5_dice_value: 0.9946 - val_main_out_dice_value: 0.9968\n",
      "Epoch 4/150\n",
      " 2993/24032 [==>...........................] - ETA: 11631s - loss: 0.5985 - aux_out4_loss: 0.4614 - aux_out5_loss: 0.4662 - main_out_loss: 0.4829 - aux_out4_dice_value: 0.9958 - aux_out5_dice_value: 0.9946 - main_out_dice_value: 0.9970"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-c569a1c068d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m                     validation_steps=np.ceil(float(len(ids_valid_split)) / float(batch_size)))\n\u001b[0m",
      "\u001b[0;32m/home/dariush/anaconda3/envs/deep1/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dariush/anaconda3/envs/deep1/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2037\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   2038\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2039\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   2040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2041\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dariush/anaconda3/envs/deep1/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1757\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1758\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1759\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1760\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1761\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dariush/anaconda3/envs/deep1/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2268\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2269\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2270\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2271\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dariush/anaconda3/envs/deep1/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dariush/anaconda3/envs/deep1/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dariush/anaconda3/envs/deep1/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/dariush/anaconda3/envs/deep1/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dariush/anaconda3/envs/deep1/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 150\n",
    "batch_size = 1\n",
    "accum_iters = 32\n",
    "auxiliaries = [False, False, False, False, True, True]\n",
    "outputs = {'aux_out4':2**-4, 'aux_out5':2**-5, 'main_out':1}\n",
    "weights = {'aux_out4':0.2, 'aux_out5':0.05, 'main_out':1.}\n",
    "\n",
    "model = UNet_Heng_Aux((input_size, input_size, 3), auxiliaries=auxiliaries)\n",
    "model.compile(optimizer=AdamAccumulate(accum_iters=accum_iters), \n",
    "              loss=weighted_bce_dice_loss, metrics=[dice_value], loss_weights=weights)\n",
    "run_name = utils.get_run_name('weights/final/{}.hdf5', 'unet-ultimate')\n",
    "weights_path = 'weights/final/{}.hdf5'.format(run_name)\n",
    "\n",
    "callbacks = [EarlyStopping(monitor='val_main_out_dice_value',\n",
    "                           patience=3,\n",
    "                           verbose=1,\n",
    "                           min_delta=1e-4,\n",
    "                           mode='max'),\n",
    "             ReduceLROnPlateau(monitor='val_main_out_dice_value',\n",
    "                               factor=0.1,\n",
    "                               patience=2,\n",
    "                               verbose=1,\n",
    "                               epsilon=1e-4,\n",
    "                               mode='max'),\n",
    "             ModelCheckpoint(monitor='val_main_out_dice_value',\n",
    "                             filepath=weights_path,\n",
    "                             save_best_only=True,\n",
    "                             save_weights_only=True,\n",
    "                             mode='max'),\n",
    "             TensorBoard(log_dir='logs/{}'.format(run_name), batch_size=batch_size)]\n",
    "\n",
    "model.load_weights('weights/unet-heng-aux-2017-09-25-1603.hdf5')\n",
    "K.set_value(model.optimizer.lr, 1e-4)\n",
    "\n",
    "pseudo_ids = get_pseudo_ids(batch_size, accum_iters)\n",
    "print('Starting run \"{}\"'.format(run_name))\n",
    "model.fit_generator(generator=pseudo_generator(batch_size, pseudo_ids, outputs), \n",
    "                    steps_per_epoch=np.ceil(float(len(pseudo_ids)) / float(batch_size)),\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    callbacks=callbacks,\n",
    "                    validation_data=valid_generator(batch_size, outputs),\n",
    "                    validation_steps=np.ceil(float(len(ids_valid_split)) / float(batch_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Mask Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import cv2\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.load_weights('weights/unet-2017-09-02-1809.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "imgs_names = glob('inputs/train/*.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "num = 1\n",
    "input_size = 1024\n",
    "pred_mask_path = \"outputs/pred_train_masks/{}_mask.gif\"\n",
    "for i in range(len(imgs_names)):\n",
    "    im = cv2.imread(imgs_names[i])\n",
    "    im = cv2.resize(im, (input_size, input_size), interpolation=cv2.INTER_LINEAR)\n",
    "    im = np.expand_dims(im, axis=0)\n",
    "    im = np.array(im, np.float32) / 255\n",
    "    #print im.shape\n",
    "    pred_mask = model.predict(im, batch_size=1, verbose=2)\n",
    "    #print pred_mask.shape\n",
    "    pred_mask = np.squeeze(pred_mask, axis=[0, 3])\n",
    "    #print pred_mask.shape\n",
    "    pred_mask = cv2.resize(pred_mask, (1918, 1280), interpolation=cv2.INTER_LINEAR)\n",
    "    #print pred_mask.shape\n",
    "    pred_mask = (pred_mask > 0.95)\n",
    "    pred_mask = Image.fromarray((pred_mask * 255).astype(np.uint8), mode='L')\n",
    "    im_name = imgs_names[i].split('/')[-1]\n",
    "    im_id = im_name.split('.')[0]\n",
    "    pred_mask.save(pred_mask_path.format(im_id))\n",
    "    \n",
    "    if num%500==0:\n",
    "        print '{0}/{1}'.format(num, len(imgs_names))\n",
    "    num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#val_imgs, _ = next(valid_generator(len(ids_valid_split)))\n",
    "train_imgs, _ = next(train_generator(len(ids_train_split[4000:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#print val_imgs.shape\n",
    "print train_imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#val_pred_masks = model.predict(val_imgs, batch_size=16, verbose=1)\n",
    "train_pred_masks = model.predict(train_imgs, batch_size=16, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "preds = np.squeeze(train_pred_masks, axis=3)\n",
    "np.shape(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pred_mask_path = \"outputs/valid_masks/{}_mask.gif\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for i in range(train_imgs.shape[0]):\n",
    "    mask = preds[i]\n",
    "    mask = cv2.resize(mask, (1918, 1280), interpolation=cv2.INTER_LINEAR)\n",
    "    mask = (mask > 0.95)\n",
    "    mask = Image.fromarray((mask * 255).astype(np.uint8), mode='L')\n",
    "    mask.save(pred_mask_path.format(ids_valid_split.values[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def np_dice_value(y_true, y_pred):\n",
    "    smooth = 1.\n",
    "    y_true_f = y_true.flatten()\n",
    "    y_pred_f = y_pred.flatten()\n",
    "    intersection = np.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (np.sum(y_true_f) + np.sum(y_pred_f) + smooth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_name = 'unet-2017-09-03-1739'\n",
    "model.load_weights('weights/{}.hdf5'.format(run_name))\n",
    "\n",
    "val_imgs, val_masks = next(valid_generator(len(ids_valid_split)))\n",
    "val_imgs = np.array(val_imgs)\n",
    "val_masks = np.array(val_masks)\n",
    "val_pred_masks = model.predict(val_imgs, batch_size=1)\n",
    "masks_val_dices = [np_dice_value(mask, pred_mask) for (mask, pred_mask) in zip(val_masks, val_pred_masks)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the worst predicted mask for validation examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index = np.argsort(masks_val_dices)[7]\n",
    "id = ids_valid_split.values[index]\n",
    "utils.show_mask(train_path.format(id), val_masks[index].squeeze(), val_pred_masks[index].squeeze(), show_img=False)\n",
    "print id, masks_val_dices[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indices = np.argsort(masks_val_dices[masks_val_dices <= 99.6])\n",
    "for id in indices:\n",
    "        print(masks_val_dices[id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hist, bins = np.histogram(masks_val_dices, bins=50)\n",
    "width = 0.7 * (bins[1] - bins[0])\n",
    "center = (bins[:-1] + bins[1:]) / 2\n",
    "plt.bar(center, hist, align='center', width=width)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indices = np.random.randint(len(ids_valid_split), size=3)\n",
    "for index in indices:\n",
    "    id = ids_valid_split.values[index]\n",
    "    utils.show_mask(train_path.format(id), val_masks[index].squeeze(), val_pred_masks[index].squeeze(),\n",
    "                    show_img=True, bbox = bboxes[id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Create model first if required\n",
    "run_name = 'unet-2017-08-20-5'\n",
    "model.load_weights('weights/{}.hdf5'.format(run_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Generate Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "threshold = 0.5\n",
    "test_path = 'inputs/test1/' #'inputs/test/'\n",
    "test_masks_path = 'outputs/test1_masks/' #None\n",
    "generate_submit(model, input_size, batch_size, threshold, test_path, 'outputs/', run_name, test_masks_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "utils.show_test_masks(test_path, test_masks_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
