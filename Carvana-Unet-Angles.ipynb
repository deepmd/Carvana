{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carvana U-Net Angles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TensorBoard\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from optimizers.AdamAccumulate import AdamAccumulate\n",
    "from models.u_net import UNet\n",
    "from models.u_net_aux import UNet_Aux\n",
    "from models.u_net_heng import UNet_Heng\n",
    "from utilities.submit import generate_submit\n",
    "from utilities import utils_masks as utils\n",
    "from utilities.losses import weighted_bce_dice_loss, dice_value\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group 01:   #Training = 259   #Validation = 59\n",
      "group 02:   #Training = 260   #Validation = 58\n",
      "group 03:   #Training = 259   #Validation = 59\n",
      "group 04:   #Training = 257   #Validation = 61\n",
      "group 05:   #Training = 256   #Validation = 62\n",
      "group 06:   #Training = 249   #Validation = 69\n",
      "group 07:   #Training = 259   #Validation = 59\n",
      "group 08:   #Training = 257   #Validation = 61\n",
      "group 09:   #Training = 247   #Validation = 71\n",
      "group 10:   #Training = 254   #Validation = 64\n",
      "group 11:   #Training = 248   #Validation = 70\n",
      "group 12:   #Training = 259   #Validation = 59\n",
      "group 13:   #Training = 252   #Validation = 66\n",
      "group 14:   #Training = 254   #Validation = 64\n",
      "group 15:   #Training = 261   #Validation = 57\n",
      "group 16:   #Training = 239   #Validation = 79\n"
     ]
    }
   ],
   "source": [
    "utils.set_results_reproducible()\n",
    "input_size = 128\n",
    "train_path = \"inputs/train/{}.jpg\" \n",
    "train_mask_path = \"inputs/train_masks/{}_mask.gif\"\n",
    "\n",
    "df = pd.read_csv('inputs/train_masks.csv')\n",
    "all_ids = df['img'].map(lambda s: s.split('.')[0])\n",
    "all_ids_train_split, all_ids_valid_split = train_test_split(all_ids, test_size=0.2, random_state=42)\n",
    "\n",
    "groups = ['01','02','03','04','05','06','07','08','09','10','11','12','13','14','15','16']\n",
    "ids_train_splits = {}\n",
    "ids_valid_splits = {}\n",
    "\n",
    "for group in groups:\n",
    "    df_group = df[(df.img.str.match('^.*_(' + group + ').jpg$'))]\n",
    "    ids_group = df_group['img'].map(lambda s: s.split('.')[0])\n",
    "    ids_train_split = pd.Series(list(set(all_ids_train_split).intersection(set(ids_group))))\n",
    "    ids_valid_split = pd.Series(list(set(all_ids_valid_split).intersection(set(ids_group))))\n",
    "    ids_train_splits[group] = ids_train_split\n",
    "    ids_valid_splits[group] = ids_valid_split\n",
    "    print('group {0}:   #Training = {1}   #Validation = {2}'.format(group, len(ids_train_split), len(ids_valid_split)))\n",
    "\n",
    "#bboxes = None\n",
    "bbox_file_path = 'inputs/train_bbox.csv'\n",
    "bboxes = utils.get_bboxes(bbox_file_path)\n",
    "\n",
    "def train_generator(batch_size, group, outputs=None):\n",
    "    return utils.train_generator(train_path, train_mask_path, ids_train_splits[group], \n",
    "                                 input_size, batch_size, bboxes, outputs=outputs,\n",
    "                                 augmentations=['HUE_SATURATION', 'SHIFT_SCALE'])\n",
    "\n",
    "def valid_generator(batch_size, group, outputs=None):\n",
    "    return utils.valid_generator(train_path, train_mask_path, ids_valid_splits[group],\n",
    "                                 input_size, batch_size, bboxes, outputs=outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#U-Net-Aux:\n",
    "#model = UNet_Aux((input_size, input_size, 3), filters=64, depth=4, dropout_base_only=False, dropout=0,\n",
    "#                 activation=lambda x: PReLU()(x), init='he_uniform', auxiliaries=[False, True, True, False])\n",
    "#outputs = {'aux_out1':2**-1, 'aux_out2':2**-2, 'main_out':1}\n",
    "#weights = {'aux_out1':0.2, 'aux_out2':0.05, 'main_out':1.}\n",
    "#model.compile(optimizer=AdamAccumulate(accum_iters=4), \n",
    "#              loss=weighted_bce_dice_loss, metrics=[dice_value], loss_weights=weights)\n",
    "\n",
    "#U-Net:\n",
    "#model = UNet((input_size, input_size, 3), filters=64, depth=4, dropout_base_only=False, dropout=0,\n",
    "#             activation=lambda x: PReLU()(x), init='he_uniform')\n",
    "#model.compile(optimizer=AdamAccumulate(accum_iters=4), loss=weighted_bce_dice_loss, metrics=[dice_value])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting run \"unet-(01)-2017-09-25-0621\"\n",
      "Epoch 1/150\n",
      "259/259 [==============================] - 29s - loss: 0.4318 - dice_value: 0.9931 - val_loss: 0.4140 - val_dice_value: 0.9958\n",
      "Epoch 2/150\n",
      "259/259 [==============================] - 25s - loss: 0.4277 - dice_value: 0.9935 - val_loss: 0.4142 - val_dice_value: 0.9957\n",
      "Epoch 3/150\n",
      "259/259 [==============================] - 25s - loss: 0.4283 - dice_value: 0.9941 - val_loss: 0.4141 - val_dice_value: 0.9957\n",
      "Epoch 4/150\n",
      "259/259 [==============================] - 25s - loss: 0.4292 - dice_value: 0.9940 - val_loss: 0.4141 - val_dice_value: 0.9957\n",
      "Epoch 5/150\n",
      "259/259 [==============================] - 25s - loss: 0.4273 - dice_value: 0.9941 - val_loss: 0.4144 - val_dice_value: 0.9957\n",
      "Epoch 6/150\n",
      "258/259 [============================>.] - ETA: 0s - loss: 0.4303 - dice_value: 0.9937\n",
      "Epoch 00005: reducing learning rate to 9.999999747378752e-06.\n",
      "259/259 [==============================] - 25s - loss: 0.4303 - dice_value: 0.9937 - val_loss: 0.4141 - val_dice_value: 0.9957\n",
      "Epoch 7/150\n",
      "259/259 [==============================] - 26s - loss: 0.4272 - dice_value: 0.9942 - val_loss: 0.4138 - val_dice_value: 0.9958\n",
      "Epoch 8/150\n",
      "259/259 [==============================] - 25s - loss: 0.4252 - dice_value: 0.9944 - val_loss: 0.4138 - val_dice_value: 0.9958\n",
      "Epoch 9/150\n",
      "259/259 [==============================] - 25s - loss: 0.4271 - dice_value: 0.9947 - val_loss: 0.4137 - val_dice_value: 0.9958\n",
      "Epoch 10/150\n",
      "258/259 [============================>.] - ETA: 0s - loss: 0.4247 - dice_value: 0.9940\n",
      "Epoch 00009: reducing learning rate to 9.999999747378752e-07.\n",
      "259/259 [==============================] - 25s - loss: 0.4246 - dice_value: 0.9940 - val_loss: 0.4139 - val_dice_value: 0.9958\n",
      "Epoch 00009: early stopping\n",
      "\n",
      "Starting run \"unet-(02)-2017-09-25-0626\"\n",
      "Epoch 1/150\n",
      "260/260 [==============================] - 28s - loss: 0.4565 - dice_value: 0.9944 - val_loss: 0.4479 - val_dice_value: 0.9960\n",
      "Epoch 2/150\n",
      "260/260 [==============================] - 25s - loss: 0.4552 - dice_value: 0.9944 - val_loss: 0.4479 - val_dice_value: 0.9960\n",
      "Epoch 3/150\n",
      "260/260 [==============================] - 25s - loss: 0.4527 - dice_value: 0.9947 - val_loss: 0.4481 - val_dice_value: 0.9959\n",
      "Epoch 4/150\n",
      "260/260 [==============================] - 26s - loss: 0.4550 - dice_value: 0.9948 - val_loss: 0.4478 - val_dice_value: 0.9960\n",
      "Epoch 5/150\n",
      "260/260 [==============================] - 26s - loss: 0.4535 - dice_value: 0.9948 - val_loss: 0.4476 - val_dice_value: 0.9960\n",
      "Epoch 6/150\n",
      "259/260 [============================>.] - ETA: 0s - loss: 0.4564 - dice_value: 0.9945\n",
      "Epoch 00005: reducing learning rate to 9.999999747378752e-06.\n",
      "260/260 [==============================] - 25s - loss: 0.4568 - dice_value: 0.9945 - val_loss: 0.4478 - val_dice_value: 0.9960\n",
      "Epoch 7/150\n",
      "260/260 [==============================] - 25s - loss: 0.4551 - dice_value: 0.9949 - val_loss: 0.4476 - val_dice_value: 0.9960\n",
      "Epoch 8/150\n",
      "260/260 [==============================] - 25s - loss: 0.4536 - dice_value: 0.9946 - val_loss: 0.4477 - val_dice_value: 0.9960\n",
      "Epoch 9/150\n",
      "260/260 [==============================] - 25s - loss: 0.4589 - dice_value: 0.9942 - val_loss: 0.4477 - val_dice_value: 0.9960\n",
      "Epoch 10/150\n",
      "259/260 [============================>.] - ETA: 0s - loss: 0.4546 - dice_value: 0.9948\n",
      "Epoch 00009: reducing learning rate to 9.999999747378752e-07.\n",
      "260/260 [==============================] - 25s - loss: 0.4547 - dice_value: 0.9948 - val_loss: 0.4476 - val_dice_value: 0.9960\n",
      "Epoch 00009: early stopping\n",
      "\n",
      "Starting run \"unet-(03)-2017-09-25-0631\"\n",
      "Epoch 1/150\n",
      "259/259 [==============================] - 28s - loss: 0.4890 - dice_value: 0.9939 - val_loss: 0.4755 - val_dice_value: 0.9960\n",
      "Epoch 2/150\n",
      "259/259 [==============================] - 25s - loss: 0.4853 - dice_value: 0.9943 - val_loss: 0.4758 - val_dice_value: 0.9959\n",
      "Epoch 3/150\n",
      "259/259 [==============================] - 25s - loss: 0.4844 - dice_value: 0.9945 - val_loss: 0.4762 - val_dice_value: 0.9958\n",
      "Epoch 4/150\n",
      "259/259 [==============================] - 25s - loss: 0.4836 - dice_value: 0.9945 - val_loss: 0.4761 - val_dice_value: 0.9958\n",
      "Epoch 5/150\n",
      "259/259 [==============================] - 25s - loss: 0.4830 - dice_value: 0.9945 - val_loss: 0.4757 - val_dice_value: 0.9959\n",
      "Epoch 6/150\n",
      "258/259 [============================>.] - ETA: 0s - loss: 0.4857 - dice_value: 0.9944\n",
      "Epoch 00005: reducing learning rate to 9.999999747378752e-06.\n",
      "259/259 [==============================] - 25s - loss: 0.4857 - dice_value: 0.9944 - val_loss: 0.4760 - val_dice_value: 0.9959\n",
      "Epoch 7/150\n",
      "259/259 [==============================] - 25s - loss: 0.4850 - dice_value: 0.9946 - val_loss: 0.4754 - val_dice_value: 0.9960\n",
      "Epoch 8/150\n",
      "259/259 [==============================] - 26s - loss: 0.4829 - dice_value: 0.9947 - val_loss: 0.4753 - val_dice_value: 0.9960\n",
      "Epoch 9/150\n",
      "259/259 [==============================] - 26s - loss: 0.4839 - dice_value: 0.9947 - val_loss: 0.4753 - val_dice_value: 0.9960\n",
      "Epoch 10/150\n",
      "258/259 [============================>.] - ETA: 0s - loss: 0.4850 - dice_value: 0.9945\n",
      "Epoch 00009: reducing learning rate to 9.999999747378752e-07.\n",
      "259/259 [==============================] - 26s - loss: 0.4851 - dice_value: 0.9945 - val_loss: 0.4753 - val_dice_value: 0.9961\n",
      "Epoch 00009: early stopping\n",
      "\n",
      "Starting run \"unet-(04)-2017-09-25-0636\"\n",
      "Epoch 1/150\n",
      "257/257 [==============================] - 28s - loss: 0.4945 - dice_value: 0.9932 - val_loss: 0.4860 - val_dice_value: 0.9951\n",
      "Epoch 2/150\n",
      "257/257 [==============================] - 26s - loss: 0.4950 - dice_value: 0.9931 - val_loss: 0.4859 - val_dice_value: 0.9952\n",
      "Epoch 3/150\n",
      "257/257 [==============================] - 26s - loss: 0.4914 - dice_value: 0.9936 - val_loss: 0.4859 - val_dice_value: 0.9952\n",
      "Epoch 4/150\n",
      "257/257 [==============================] - 26s - loss: 0.4925 - dice_value: 0.9938 - val_loss: 0.4858 - val_dice_value: 0.9952\n",
      "Epoch 5/150\n",
      "257/257 [==============================] - 25s - loss: 0.4928 - dice_value: 0.9939 - val_loss: 0.4860 - val_dice_value: 0.9951\n",
      "Epoch 6/150\n",
      "256/257 [============================>.] - ETA: 0s - loss: 0.4920 - dice_value: 0.9938\n",
      "Epoch 00005: reducing learning rate to 9.999999747378752e-06.\n",
      "257/257 [==============================] - 25s - loss: 0.4920 - dice_value: 0.9938 - val_loss: 0.4859 - val_dice_value: 0.9952\n",
      "Epoch 7/150\n",
      "257/257 [==============================] - 25s - loss: 0.4912 - dice_value: 0.9941 - val_loss: 0.4859 - val_dice_value: 0.9952\n",
      "Epoch 8/150\n",
      "257/257 [==============================] - 25s - loss: 0.4919 - dice_value: 0.9939 - val_loss: 0.4858 - val_dice_value: 0.9952\n",
      "Epoch 9/150\n",
      "257/257 [==============================] - 25s - loss: 0.4929 - dice_value: 0.9939 - val_loss: 0.4858 - val_dice_value: 0.9952\n",
      "Epoch 10/150\n",
      "256/257 [============================>.] - ETA: 0s - loss: 0.4940 - dice_value: 0.9937\n",
      "Epoch 00009: reducing learning rate to 9.999999747378752e-07.\n",
      "257/257 [==============================] - 25s - loss: 0.4941 - dice_value: 0.9937 - val_loss: 0.4858 - val_dice_value: 0.9952\n",
      "Epoch 00009: early stopping\n",
      "\n",
      "Starting run \"unet-(05)-2017-09-25-0640\"\n",
      "Epoch 1/150\n",
      "256/256 [==============================] - 26s - loss: 0.4809 - dice_value: 0.9931 - val_loss: 0.4719 - val_dice_value: 0.9949\n",
      "Epoch 2/150\n",
      "256/256 [==============================] - 24s - loss: 0.4818 - dice_value: 0.9929 - val_loss: 0.4719 - val_dice_value: 0.9949\n",
      "Epoch 3/150\n",
      "256/256 [==============================] - 24s - loss: 0.4793 - dice_value: 0.9935 - val_loss: 0.4721 - val_dice_value: 0.9948\n",
      "Epoch 4/150\n",
      "256/256 [==============================] - 24s - loss: 0.4819 - dice_value: 0.9931 - val_loss: 0.4720 - val_dice_value: 0.9949\n",
      "Epoch 5/150\n",
      "256/256 [==============================] - 24s - loss: 0.4810 - dice_value: 0.9934 - val_loss: 0.4719 - val_dice_value: 0.9949\n",
      "Epoch 6/150\n",
      "255/256 [============================>.] - ETA: 0s - loss: 0.4809 - dice_value: 0.9935\n",
      "Epoch 00005: reducing learning rate to 9.999999747378752e-06.\n",
      "256/256 [==============================] - 24s - loss: 0.4811 - dice_value: 0.9934 - val_loss: 0.4720 - val_dice_value: 0.9949\n",
      "Epoch 7/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256/256 [==============================] - 24s - loss: 0.4829 - dice_value: 0.9933 - val_loss: 0.4718 - val_dice_value: 0.9949\n",
      "Epoch 8/150\n",
      "256/256 [==============================] - 24s - loss: 0.4797 - dice_value: 0.9934 - val_loss: 0.4718 - val_dice_value: 0.9949\n",
      "Epoch 9/150\n",
      "256/256 [==============================] - 24s - loss: 0.4806 - dice_value: 0.9935 - val_loss: 0.4718 - val_dice_value: 0.9949\n",
      "Epoch 10/150\n",
      "255/256 [============================>.] - ETA: 0s - loss: 0.4808 - dice_value: 0.9935\n",
      "Epoch 00009: reducing learning rate to 9.999999747378752e-07.\n",
      "256/256 [==============================] - 24s - loss: 0.4808 - dice_value: 0.9935 - val_loss: 0.4718 - val_dice_value: 0.9949\n",
      "Epoch 00009: early stopping\n",
      "\n",
      "Starting run \"unet-(06)-2017-09-25-0645\"\n",
      "Epoch 1/150\n",
      "249/249 [==============================] - 26s - loss: 0.4917 - dice_value: 0.9931 - val_loss: 0.4797 - val_dice_value: 0.9956\n",
      "Epoch 2/150\n",
      "249/249 [==============================] - 23s - loss: 0.4887 - dice_value: 0.9935 - val_loss: 0.4797 - val_dice_value: 0.9955\n",
      "Epoch 3/150\n",
      "249/249 [==============================] - 23s - loss: 0.4885 - dice_value: 0.9937 - val_loss: 0.4800 - val_dice_value: 0.9955\n",
      "Epoch 4/150\n",
      "249/249 [==============================] - 23s - loss: 0.4901 - dice_value: 0.9936 - val_loss: 0.4798 - val_dice_value: 0.9955\n",
      "Epoch 5/150\n",
      "249/249 [==============================] - 23s - loss: 0.4887 - dice_value: 0.9939 - val_loss: 0.4798 - val_dice_value: 0.9955\n",
      "Epoch 6/150\n",
      "248/249 [============================>.] - ETA: 0s - loss: 0.4872 - dice_value: 0.9939\n",
      "Epoch 00005: reducing learning rate to 9.999999747378752e-06.\n",
      "249/249 [==============================] - 23s - loss: 0.4871 - dice_value: 0.9939 - val_loss: 0.4798 - val_dice_value: 0.9955\n",
      "Epoch 7/150\n",
      "249/249 [==============================] - 23s - loss: 0.4865 - dice_value: 0.9939 - val_loss: 0.4798 - val_dice_value: 0.9955\n",
      "Epoch 8/150\n",
      "249/249 [==============================] - 23s - loss: 0.4881 - dice_value: 0.9939 - val_loss: 0.4798 - val_dice_value: 0.9955\n",
      "Epoch 9/150\n",
      "249/249 [==============================] - 23s - loss: 0.4909 - dice_value: 0.9936 - val_loss: 0.4798 - val_dice_value: 0.9955\n",
      "Epoch 10/150\n",
      "248/249 [============================>.] - ETA: 0s - loss: 0.4861 - dice_value: 0.9940\n",
      "Epoch 00009: reducing learning rate to 9.999999747378752e-07.\n",
      "249/249 [==============================] - 23s - loss: 0.4863 - dice_value: 0.9940 - val_loss: 0.4798 - val_dice_value: 0.9955\n",
      "Epoch 00009: early stopping\n",
      "\n",
      "Starting run \"unet-(07)-2017-09-25-0649\"\n",
      "Epoch 1/150\n",
      "259/259 [==============================] - 28s - loss: 0.4825 - dice_value: 0.9941 - val_loss: 0.4708 - val_dice_value: 0.9963\n",
      "Epoch 2/150\n",
      "259/259 [==============================] - 25s - loss: 0.4834 - dice_value: 0.9941 - val_loss: 0.4711 - val_dice_value: 0.9962\n",
      "Epoch 3/150\n",
      "259/259 [==============================] - 26s - loss: 0.4808 - dice_value: 0.9944 - val_loss: 0.4707 - val_dice_value: 0.9963\n",
      "Epoch 4/150\n",
      "259/259 [==============================] - 25s - loss: 0.4822 - dice_value: 0.9944 - val_loss: 0.4709 - val_dice_value: 0.9963\n",
      "Epoch 5/150\n",
      "259/259 [==============================] - 25s - loss: 0.4814 - dice_value: 0.9947 - val_loss: 0.4709 - val_dice_value: 0.9963\n",
      "Epoch 6/150\n",
      "258/259 [============================>.] - ETA: 0s - loss: 0.4784 - dice_value: 0.9948\n",
      "Epoch 00005: reducing learning rate to 9.999999747378752e-06.\n",
      "259/259 [==============================] - 25s - loss: 0.4785 - dice_value: 0.9948 - val_loss: 0.4709 - val_dice_value: 0.9963\n",
      "Epoch 7/150\n",
      "259/259 [==============================] - 26s - loss: 0.4812 - dice_value: 0.9947 - val_loss: 0.4707 - val_dice_value: 0.9963\n",
      "Epoch 8/150\n",
      "259/259 [==============================] - 26s - loss: 0.4786 - dice_value: 0.9949 - val_loss: 0.4707 - val_dice_value: 0.9964\n",
      "Epoch 9/150\n",
      "259/259 [==============================] - 25s - loss: 0.4802 - dice_value: 0.9948 - val_loss: 0.4707 - val_dice_value: 0.9963\n",
      "Epoch 10/150\n",
      "258/259 [============================>.] - ETA: 0s - loss: 0.4799 - dice_value: 0.9947\n",
      "Epoch 00009: reducing learning rate to 9.999999747378752e-07.\n",
      "259/259 [==============================] - 26s - loss: 0.4799 - dice_value: 0.9947 - val_loss: 0.4707 - val_dice_value: 0.9964\n",
      "Epoch 00009: early stopping\n",
      "\n",
      "Starting run \"unet-(08)-2017-09-25-0654\"\n",
      "Epoch 1/150\n",
      "257/257 [==============================] - 27s - loss: 0.4608 - dice_value: 0.9940 - val_loss: 0.4530 - val_dice_value: 0.9959\n",
      "Epoch 2/150\n",
      "257/257 [==============================] - 25s - loss: 0.4605 - dice_value: 0.9939 - val_loss: 0.4530 - val_dice_value: 0.9960\n",
      "Epoch 3/150\n",
      "257/257 [==============================] - 25s - loss: 0.4599 - dice_value: 0.9946 - val_loss: 0.4531 - val_dice_value: 0.9959\n",
      "Epoch 4/150\n",
      "257/257 [==============================] - 25s - loss: 0.4620 - dice_value: 0.9937 - val_loss: 0.4530 - val_dice_value: 0.9960\n",
      "Epoch 5/150\n",
      "257/257 [==============================] - 25s - loss: 0.4612 - dice_value: 0.9944 - val_loss: 0.4529 - val_dice_value: 0.9960\n",
      "Epoch 6/150\n",
      "256/257 [============================>.] - ETA: 0s - loss: 0.4623 - dice_value: 0.9944\n",
      "Epoch 00005: reducing learning rate to 9.999999747378752e-06.\n",
      "257/257 [==============================] - 25s - loss: 0.4623 - dice_value: 0.9944 - val_loss: 0.4530 - val_dice_value: 0.9960\n",
      "Epoch 7/150\n",
      "257/257 [==============================] - 25s - loss: 0.4600 - dice_value: 0.9949 - val_loss: 0.4528 - val_dice_value: 0.9960\n",
      "Epoch 8/150\n",
      "257/257 [==============================] - 25s - loss: 0.4603 - dice_value: 0.9946 - val_loss: 0.4528 - val_dice_value: 0.9960\n",
      "Epoch 9/150\n",
      "257/257 [==============================] - 25s - loss: 0.4597 - dice_value: 0.9946 - val_loss: 0.4527 - val_dice_value: 0.9960\n",
      "Epoch 10/150\n",
      "256/257 [============================>.] - ETA: 0s - loss: 0.4620 - dice_value: 0.9944\n",
      "Epoch 00009: reducing learning rate to 9.999999747378752e-07.\n",
      "257/257 [==============================] - 25s - loss: 0.4621 - dice_value: 0.9944 - val_loss: 0.4527 - val_dice_value: 0.9960\n",
      "Epoch 00009: early stopping\n",
      "\n",
      "Starting run \"unet-(09)-2017-09-25-0659\"\n",
      "Epoch 1/150\n",
      "247/247 [==============================] - 26s - loss: 0.4413 - dice_value: 0.9923 - val_loss: 0.4249 - val_dice_value: 0.9954\n",
      "Epoch 2/150\n",
      "247/247 [==============================] - 23s - loss: 0.4397 - dice_value: 0.9936 - val_loss: 0.4252 - val_dice_value: 0.9953\n",
      "Epoch 3/150\n",
      "247/247 [==============================] - 24s - loss: 0.4372 - dice_value: 0.9937 - val_loss: 0.4250 - val_dice_value: 0.9954\n",
      "Epoch 4/150\n",
      "247/247 [==============================] - 23s - loss: 0.4367 - dice_value: 0.9936 - val_loss: 0.4263 - val_dice_value: 0.9940\n",
      "Epoch 5/150\n",
      "247/247 [==============================] - 23s - loss: 0.4349 - dice_value: 0.9937 - val_loss: 0.4266 - val_dice_value: 0.9940\n",
      "Epoch 6/150\n",
      "246/247 [============================>.] - ETA: 0s - loss: 0.4383 - dice_value: 0.9938\n",
      "Epoch 00005: reducing learning rate to 9.999999747378752e-06.\n",
      "247/247 [==============================] - 23s - loss: 0.4384 - dice_value: 0.9938 - val_loss: 0.4263 - val_dice_value: 0.9941\n",
      "Epoch 7/150\n",
      "247/247 [==============================] - 23s - loss: 0.4380 - dice_value: 0.9936 - val_loss: 0.4263 - val_dice_value: 0.9939\n",
      "Epoch 8/150\n",
      "247/247 [==============================] - 23s - loss: 0.4365 - dice_value: 0.9939 - val_loss: 0.4263 - val_dice_value: 0.9940\n",
      "Epoch 9/150\n",
      "247/247 [==============================] - 23s - loss: 0.4393 - dice_value: 0.9939 - val_loss: 0.4260 - val_dice_value: 0.9943\n",
      "Epoch 10/150\n",
      "246/247 [============================>.] - ETA: 0s - loss: 0.4332 - dice_value: 0.9944\n",
      "Epoch 00009: reducing learning rate to 9.999999747378752e-07.\n",
      "247/247 [==============================] - 23s - loss: 0.4331 - dice_value: 0.9944 - val_loss: 0.4267 - val_dice_value: 0.9938\n",
      "Epoch 00009: early stopping\n",
      "\n",
      "Starting run \"unet-(10)-2017-09-25-0703\"\n",
      "Epoch 1/150\n",
      "254/254 [==============================] - 27s - loss: 0.4532 - dice_value: 0.9938 - val_loss: 0.4491 - val_dice_value: 0.9959\n",
      "Epoch 2/150\n",
      "254/254 [==============================] - 25s - loss: 0.4541 - dice_value: 0.9944 - val_loss: 0.4491 - val_dice_value: 0.9959\n",
      "Epoch 3/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "254/254 [==============================] - 25s - loss: 0.4567 - dice_value: 0.9942 - val_loss: 0.4493 - val_dice_value: 0.9958\n",
      "Epoch 4/150\n",
      "254/254 [==============================] - 24s - loss: 0.4547 - dice_value: 0.9940 - val_loss: 0.4493 - val_dice_value: 0.9958\n",
      "Epoch 5/150\n",
      "254/254 [==============================] - 25s - loss: 0.4547 - dice_value: 0.9942 - val_loss: 0.4490 - val_dice_value: 0.9959\n",
      "Epoch 6/150\n",
      "253/254 [============================>.] - ETA: 0s - loss: 0.4539 - dice_value: 0.9946\n",
      "Epoch 00005: reducing learning rate to 9.999999747378752e-06.\n",
      "254/254 [==============================] - 24s - loss: 0.4538 - dice_value: 0.9946 - val_loss: 0.4491 - val_dice_value: 0.9959\n",
      "Epoch 7/150\n",
      "254/254 [==============================] - 25s - loss: 0.4531 - dice_value: 0.9946 - val_loss: 0.4490 - val_dice_value: 0.9959\n",
      "Epoch 8/150\n",
      "254/254 [==============================] - 25s - loss: 0.4519 - dice_value: 0.9947 - val_loss: 0.4489 - val_dice_value: 0.9959\n",
      "Epoch 9/150\n",
      "254/254 [==============================] - 24s - loss: 0.4547 - dice_value: 0.9944 - val_loss: 0.4489 - val_dice_value: 0.9959\n",
      "Epoch 10/150\n",
      "253/254 [============================>.] - ETA: 0s - loss: 0.4523 - dice_value: 0.9947\n",
      "Epoch 00009: reducing learning rate to 9.999999747378752e-07.\n",
      "254/254 [==============================] - 25s - loss: 0.4525 - dice_value: 0.9947 - val_loss: 0.4489 - val_dice_value: 0.9959\n",
      "Epoch 00009: early stopping\n",
      "\n",
      "Starting run \"unet-(11)-2017-09-25-0708\"\n",
      "Epoch 1/150\n",
      "248/248 [==============================] - 27s - loss: 0.4806 - dice_value: 0.9939 - val_loss: 0.4678 - val_dice_value: 0.9960\n",
      "Epoch 2/150\n",
      "248/248 [==============================] - 24s - loss: 0.4807 - dice_value: 0.9939 - val_loss: 0.4681 - val_dice_value: 0.9959\n",
      "Epoch 3/150\n",
      "248/248 [==============================] - 24s - loss: 0.4785 - dice_value: 0.9944 - val_loss: 0.4678 - val_dice_value: 0.9959\n",
      "Epoch 4/150\n",
      "248/248 [==============================] - 24s - loss: 0.4799 - dice_value: 0.9943 - val_loss: 0.4679 - val_dice_value: 0.9959\n",
      "Epoch 5/150\n",
      "248/248 [==============================] - 24s - loss: 0.4797 - dice_value: 0.9943 - val_loss: 0.4678 - val_dice_value: 0.9960\n",
      "Epoch 6/150\n",
      "247/248 [============================>.] - ETA: 0s - loss: 0.4784 - dice_value: 0.9943\n",
      "Epoch 00005: reducing learning rate to 9.999999747378752e-06.\n",
      "248/248 [==============================] - 24s - loss: 0.4786 - dice_value: 0.9943 - val_loss: 0.4680 - val_dice_value: 0.9959\n",
      "Epoch 7/150\n",
      "248/248 [==============================] - 25s - loss: 0.4767 - dice_value: 0.9947 - val_loss: 0.4676 - val_dice_value: 0.9960\n",
      "Epoch 8/150\n",
      "248/248 [==============================] - 25s - loss: 0.4753 - dice_value: 0.9948 - val_loss: 0.4675 - val_dice_value: 0.9960\n",
      "Epoch 9/150\n",
      "248/248 [==============================] - 24s - loss: 0.4770 - dice_value: 0.9944 - val_loss: 0.4675 - val_dice_value: 0.9960\n",
      "Epoch 10/150\n",
      "247/248 [============================>.] - ETA: 0s - loss: 0.4768 - dice_value: 0.9948\n",
      "Epoch 00009: reducing learning rate to 9.999999747378752e-07.\n",
      "248/248 [==============================] - 25s - loss: 0.4767 - dice_value: 0.9948 - val_loss: 0.4675 - val_dice_value: 0.9960\n",
      "Epoch 00009: early stopping\n",
      "\n",
      "Starting run \"unet-(12)-2017-09-25-0713\"\n",
      "Epoch 1/150\n",
      "259/259 [==============================] - 26s - loss: 0.4882 - dice_value: 0.9937 - val_loss: 0.4817 - val_dice_value: 0.9958\n",
      "Epoch 2/150\n",
      "259/259 [==============================] - 24s - loss: 0.4890 - dice_value: 0.9938 - val_loss: 0.4819 - val_dice_value: 0.9958\n",
      "Epoch 3/150\n",
      "259/259 [==============================] - 24s - loss: 0.4885 - dice_value: 0.9939 - val_loss: 0.4819 - val_dice_value: 0.9958\n",
      "Epoch 4/150\n",
      "259/259 [==============================] - 25s - loss: 0.4901 - dice_value: 0.9935 - val_loss: 0.4816 - val_dice_value: 0.9959\n",
      "Epoch 5/150\n",
      "259/259 [==============================] - 24s - loss: 0.4896 - dice_value: 0.9938 - val_loss: 0.4816 - val_dice_value: 0.9959\n",
      "Epoch 6/150\n",
      "258/259 [============================>.] - ETA: 0s - loss: 0.4889 - dice_value: 0.9937\n",
      "Epoch 00005: reducing learning rate to 9.999999747378752e-06.\n",
      "259/259 [==============================] - 24s - loss: 0.4889 - dice_value: 0.9937 - val_loss: 0.4816 - val_dice_value: 0.9959\n",
      "Epoch 7/150\n",
      "259/259 [==============================] - 25s - loss: 0.4894 - dice_value: 0.9939 - val_loss: 0.4814 - val_dice_value: 0.9959\n",
      "Epoch 8/150\n",
      "259/259 [==============================] - 24s - loss: 0.4872 - dice_value: 0.9941 - val_loss: 0.4815 - val_dice_value: 0.9959\n",
      "Epoch 9/150\n",
      "259/259 [==============================] - 25s - loss: 0.4883 - dice_value: 0.9939 - val_loss: 0.4814 - val_dice_value: 0.9959\n",
      "Epoch 10/150\n",
      "258/259 [============================>.] - ETA: 0s - loss: 0.4879 - dice_value: 0.9938\n",
      "Epoch 00009: reducing learning rate to 9.999999747378752e-07.\n",
      "259/259 [==============================] - 24s - loss: 0.4879 - dice_value: 0.9938 - val_loss: 0.4815 - val_dice_value: 0.9959\n",
      "Epoch 00009: early stopping\n",
      "\n",
      "Starting run \"unet-(13)-2017-09-25-0717\"\n",
      "Epoch 1/150\n",
      "252/252 [==============================] - 27s - loss: 0.4828 - dice_value: 0.9923 - val_loss: 0.4700 - val_dice_value: 0.9948\n",
      "Epoch 2/150\n",
      "252/252 [==============================] - 24s - loss: 0.4820 - dice_value: 0.9930 - val_loss: 0.4702 - val_dice_value: 0.9947\n",
      "Epoch 3/150\n",
      "252/252 [==============================] - 25s - loss: 0.4819 - dice_value: 0.9931 - val_loss: 0.4698 - val_dice_value: 0.9948\n",
      "Epoch 4/150\n",
      "252/252 [==============================] - 24s - loss: 0.4835 - dice_value: 0.9931 - val_loss: 0.4699 - val_dice_value: 0.9948\n",
      "Epoch 5/150\n",
      "252/252 [==============================] - 25s - loss: 0.4803 - dice_value: 0.9931 - val_loss: 0.4697 - val_dice_value: 0.9949\n",
      "Epoch 6/150\n",
      "251/252 [============================>.] - ETA: 0s - loss: 0.4805 - dice_value: 0.9932\n",
      "Epoch 00005: reducing learning rate to 9.999999747378752e-06.\n",
      "252/252 [==============================] - 24s - loss: 0.4804 - dice_value: 0.9932 - val_loss: 0.4700 - val_dice_value: 0.9948\n",
      "Epoch 7/150\n",
      "252/252 [==============================] - 24s - loss: 0.4822 - dice_value: 0.9932 - val_loss: 0.4697 - val_dice_value: 0.9949\n",
      "Epoch 8/150\n",
      "252/252 [==============================] - 24s - loss: 0.4813 - dice_value: 0.9931 - val_loss: 0.4697 - val_dice_value: 0.9949\n",
      "Epoch 9/150\n",
      "252/252 [==============================] - 25s - loss: 0.4817 - dice_value: 0.9936 - val_loss: 0.4697 - val_dice_value: 0.9949\n",
      "Epoch 10/150\n",
      "251/252 [============================>.] - ETA: 0s - loss: 0.4805 - dice_value: 0.9930\n",
      "Epoch 00009: reducing learning rate to 9.999999747378752e-07.\n",
      "252/252 [==============================] - 24s - loss: 0.4807 - dice_value: 0.9930 - val_loss: 0.4697 - val_dice_value: 0.9949\n",
      "Epoch 00009: early stopping\n",
      "\n",
      "Starting run \"unet-(14)-2017-09-25-0722\"\n",
      "Epoch 1/150\n",
      "254/254 [==============================] - 27s - loss: 0.4972 - dice_value: 0.9932 - val_loss: 0.4849 - val_dice_value: 0.9953\n",
      "Epoch 2/150\n",
      "254/254 [==============================] - 25s - loss: 0.4969 - dice_value: 0.9932 - val_loss: 0.4845 - val_dice_value: 0.9954\n",
      "Epoch 3/150\n",
      "254/254 [==============================] - 25s - loss: 0.4932 - dice_value: 0.9936 - val_loss: 0.4845 - val_dice_value: 0.9954\n",
      "Epoch 4/150\n",
      "254/254 [==============================] - 25s - loss: 0.4945 - dice_value: 0.9936 - val_loss: 0.4847 - val_dice_value: 0.9953\n",
      "Epoch 5/150\n",
      "254/254 [==============================] - 25s - loss: 0.4958 - dice_value: 0.9933 - val_loss: 0.4850 - val_dice_value: 0.9952\n",
      "Epoch 6/150\n",
      "254/254 [==============================] - 25s - loss: 0.4960 - dice_value: 0.9933 - val_loss: 0.4852 - val_dice_value: 0.9952\n",
      "Epoch 7/150\n",
      "253/254 [============================>.] - ETA: 0s - loss: 0.4939 - dice_value: 0.9936\n",
      "Epoch 00006: reducing learning rate to 9.999999747378752e-06.\n",
      "254/254 [==============================] - 25s - loss: 0.4938 - dice_value: 0.9937 - val_loss: 0.4845 - val_dice_value: 0.9954\n",
      "Epoch 8/150\n",
      "254/254 [==============================] - 25s - loss: 0.4934 - dice_value: 0.9936 - val_loss: 0.4846 - val_dice_value: 0.9954\n",
      "Epoch 9/150\n",
      "254/254 [==============================] - 25s - loss: 0.4896 - dice_value: 0.9942 - val_loss: 0.4845 - val_dice_value: 0.9954\n",
      "Epoch 10/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "254/254 [==============================] - 25s - loss: 0.4920 - dice_value: 0.9940 - val_loss: 0.4845 - val_dice_value: 0.9954\n",
      "Epoch 11/150\n",
      "253/254 [============================>.] - ETA: 0s - loss: 0.4921 - dice_value: 0.9939\n",
      "Epoch 00010: reducing learning rate to 9.999999747378752e-07.\n",
      "254/254 [==============================] - 24s - loss: 0.4920 - dice_value: 0.9939 - val_loss: 0.4845 - val_dice_value: 0.9954\n",
      "Epoch 00010: early stopping\n",
      "\n",
      "Starting run \"unet-(15)-2017-09-25-0727\"\n",
      "Epoch 1/150\n",
      "261/261 [==============================] - 28s - loss: 0.4874 - dice_value: 0.9940 - val_loss: 0.4802 - val_dice_value: 0.9956\n",
      "Epoch 2/150\n",
      "261/261 [==============================] - 26s - loss: 0.4870 - dice_value: 0.9943 - val_loss: 0.4800 - val_dice_value: 0.9957\n",
      "Epoch 3/150\n",
      "261/261 [==============================] - 25s - loss: 0.4879 - dice_value: 0.9944 - val_loss: 0.4801 - val_dice_value: 0.9957\n",
      "Epoch 4/150\n",
      "261/261 [==============================] - 25s - loss: 0.4881 - dice_value: 0.9942 - val_loss: 0.4800 - val_dice_value: 0.9957\n",
      "Epoch 5/150\n",
      "261/261 [==============================] - 26s - loss: 0.4889 - dice_value: 0.9943 - val_loss: 0.4797 - val_dice_value: 0.9958\n",
      "Epoch 6/150\n",
      "261/261 [==============================] - 25s - loss: 0.4838 - dice_value: 0.9947 - val_loss: 0.4801 - val_dice_value: 0.9957\n",
      "Epoch 7/150\n",
      "261/261 [==============================] - 25s - loss: 0.4855 - dice_value: 0.9947 - val_loss: 0.5002 - val_dice_value: 0.9854\n",
      "Epoch 8/150\n",
      "261/261 [==============================] - 25s - loss: 0.4852 - dice_value: 0.9947 - val_loss: 0.4797 - val_dice_value: 0.9958\n",
      "Epoch 9/150\n",
      "261/261 [==============================] - 26s - loss: 0.4874 - dice_value: 0.9945 - val_loss: 0.4796 - val_dice_value: 0.9958\n",
      "Epoch 10/150\n",
      "261/261 [==============================] - 26s - loss: 0.4856 - dice_value: 0.9949 - val_loss: 0.4793 - val_dice_value: 0.9959\n",
      "Epoch 11/150\n",
      "261/261 [==============================] - 26s - loss: 0.4877 - dice_value: 0.9946 - val_loss: 0.4792 - val_dice_value: 0.9959\n",
      "Epoch 12/150\n",
      "261/261 [==============================] - 25s - loss: 0.4861 - dice_value: 0.9947 - val_loss: 0.4795 - val_dice_value: 0.9958\n",
      "Epoch 13/150\n",
      "261/261 [==============================] - 25s - loss: 0.4854 - dice_value: 0.9950 - val_loss: 0.4797 - val_dice_value: 0.9958\n",
      "Epoch 14/150\n",
      "261/261 [==============================] - 25s - loss: 0.4875 - dice_value: 0.9947 - val_loss: 0.4798 - val_dice_value: 0.9958\n",
      "Epoch 15/150\n",
      "260/261 [============================>.] - ETA: 0s - loss: 0.4834 - dice_value: 0.9950\n",
      "Epoch 00014: reducing learning rate to 9.999999747378752e-06.\n",
      "261/261 [==============================] - 25s - loss: 0.4834 - dice_value: 0.9950 - val_loss: 0.4793 - val_dice_value: 0.9959\n",
      "Epoch 16/150\n",
      "261/261 [==============================] - 25s - loss: 0.4837 - dice_value: 0.9949 - val_loss: 0.4793 - val_dice_value: 0.9959\n",
      "Epoch 17/150\n",
      "261/261 [==============================] - 25s - loss: 0.4836 - dice_value: 0.9948 - val_loss: 0.4793 - val_dice_value: 0.9959\n",
      "Epoch 18/150\n",
      "261/261 [==============================] - 25s - loss: 0.4849 - dice_value: 0.9950 - val_loss: 0.4794 - val_dice_value: 0.9959\n",
      "Epoch 19/150\n",
      "260/261 [============================>.] - ETA: 0s - loss: 0.4860 - dice_value: 0.9946\n",
      "Epoch 00018: reducing learning rate to 9.999999747378752e-07.\n",
      "261/261 [==============================] - 25s - loss: 0.4859 - dice_value: 0.9946 - val_loss: 0.4794 - val_dice_value: 0.9959\n",
      "Epoch 00018: early stopping\n",
      "\n",
      "Starting run \"unet-(16)-2017-09-25-0736\"\n",
      "Epoch 1/150\n",
      "239/239 [==============================] - 25s - loss: 0.4662 - dice_value: 0.9942 - val_loss: 0.4553 - val_dice_value: 0.9958\n",
      "Epoch 2/150\n",
      "239/239 [==============================] - 23s - loss: 0.4623 - dice_value: 0.9943 - val_loss: 0.4557 - val_dice_value: 0.9957\n",
      "Epoch 3/150\n",
      "239/239 [==============================] - 23s - loss: 0.4603 - dice_value: 0.9946 - val_loss: 0.4561 - val_dice_value: 0.9956\n",
      "Epoch 4/150\n",
      "239/239 [==============================] - 23s - loss: 0.4637 - dice_value: 0.9945 - val_loss: 0.4555 - val_dice_value: 0.9958\n",
      "Epoch 5/150\n",
      "239/239 [==============================] - 23s - loss: 0.4624 - dice_value: 0.9947 - val_loss: 0.4553 - val_dice_value: 0.9958\n",
      "Epoch 6/150\n",
      "238/239 [============================>.] - ETA: 0s - loss: 0.4624 - dice_value: 0.9944\n",
      "Epoch 00005: reducing learning rate to 9.999999747378752e-06.\n",
      "239/239 [==============================] - 23s - loss: 0.4625 - dice_value: 0.9944 - val_loss: 0.4556 - val_dice_value: 0.9958\n",
      "Epoch 7/150\n",
      "239/239 [==============================] - 23s - loss: 0.4616 - dice_value: 0.9948 - val_loss: 0.4553 - val_dice_value: 0.9958\n",
      "Epoch 8/150\n",
      "239/239 [==============================] - 24s - loss: 0.4606 - dice_value: 0.9947 - val_loss: 0.4553 - val_dice_value: 0.9958\n",
      "Epoch 9/150\n",
      "239/239 [==============================] - 24s - loss: 0.4610 - dice_value: 0.9949 - val_loss: 0.4552 - val_dice_value: 0.9958\n",
      "Epoch 10/150\n",
      "238/239 [============================>.] - ETA: 0s - loss: 0.4614 - dice_value: 0.9947\n",
      "Epoch 00009: reducing learning rate to 9.999999747378752e-07.\n",
      "239/239 [==============================] - 23s - loss: 0.4613 - dice_value: 0.9947 - val_loss: 0.4553 - val_dice_value: 0.9958\n",
      "Epoch 00009: early stopping\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 150\n",
    "batch_size = 1\n",
    "\n",
    "for group in groups:\n",
    "    #model = UNet_Heng((input_size, input_size, 3))\n",
    "    model = UNet((input_size, input_size, 3), filters=64, depth=4, dropout_base_only=False, dropout=0,\n",
    "             activation=lambda x: PReLU()(x), init='he_uniform')\n",
    "    model.compile(optimizer=AdamAccumulate(accum_iters=32),\n",
    "                  loss=weighted_bce_dice_loss, metrics=[dice_value])\n",
    "    run_name = utils.get_run_name('weights/{}.hdf5', 'unet-({})'.format(group))\n",
    "    weights_path = 'weights/{}.hdf5'.format(run_name)\n",
    "\n",
    "    callbacks = [EarlyStopping(monitor='val_dice_value',\n",
    "                               patience=8,\n",
    "                               verbose=1,\n",
    "                               min_delta=1e-4,\n",
    "                               mode='max'),\n",
    "                 ReduceLROnPlateau(monitor='val_dice_value',\n",
    "                                   factor=0.1,\n",
    "                                   patience=4,\n",
    "                                   verbose=1,\n",
    "                                   epsilon=1e-4,\n",
    "                                   mode='max'),\n",
    "                 ModelCheckpoint(monitor='val_dice_value',\n",
    "                                 filepath=weights_path,\n",
    "                                 save_best_only=True,\n",
    "                                 save_weights_only=True,\n",
    "                                 mode='max'),\n",
    "                 TensorBoard(log_dir='logs/{}'.format(run_name), batch_size=batch_size)]\n",
    "\n",
    "    model.load_weights('weights/unet-2017-09-22-2306.hdf5')\n",
    "    K.set_value(model.optimizer.lr, 1e-4)\n",
    "\n",
    "    print('Starting run \"{}\"'.format(run_name))\n",
    "    model.fit_generator(generator=train_generator(batch_size, group),\n",
    "                        steps_per_epoch=np.ceil(float(len(ids_train_splits[group])) / float(batch_size)),\n",
    "                        epochs=epochs,\n",
    "                        verbose=1,\n",
    "                        callbacks=callbacks,\n",
    "                        validation_data=valid_generator(batch_size, group),\n",
    "                        validation_steps=np.ceil(float(len(ids_valid_splits[group])) / float(batch_size)))\n",
    "    print()\n",
    "    K.clear_session()\n",
    "    sess = tf.Session()\n",
    "    K.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Mask Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import cv2\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.load_weights('weights/unet-2017-09-02-1809.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "imgs_names = glob('inputs/train/*.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "num = 1\n",
    "input_size = 1024\n",
    "pred_mask_path = \"outputs/pred_train_masks/{}_mask.gif\"\n",
    "for i in range(len(imgs_names)):\n",
    "    im = cv2.imread(imgs_names[i])\n",
    "    im = cv2.resize(im, (input_size, input_size), interpolation=cv2.INTER_LINEAR)\n",
    "    im = np.expand_dims(im, axis=0)\n",
    "    im = np.array(im, np.float32) / 255\n",
    "    #print im.shape\n",
    "    pred_mask = model.predict(im, batch_size=1, verbose=2)\n",
    "    #print pred_mask.shape\n",
    "    pred_mask = np.squeeze(pred_mask, axis=[0, 3])\n",
    "    #print pred_mask.shape\n",
    "    pred_mask = cv2.resize(pred_mask, (1918, 1280), interpolation=cv2.INTER_LINEAR)\n",
    "    #print pred_mask.shape\n",
    "    pred_mask = (pred_mask > 0.95)\n",
    "    pred_mask = Image.fromarray((pred_mask * 255).astype(np.uint8), mode='L')\n",
    "    im_name = imgs_names[i].split('/')[-1]\n",
    "    im_id = im_name.split('.')[0]\n",
    "    pred_mask.save(pred_mask_path.format(im_id))\n",
    "    \n",
    "    if num%500==0:\n",
    "        print '{0}/{1}'.format(num, len(imgs_names))\n",
    "    num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#val_imgs, _ = next(valid_generator(len(ids_valid_split)))\n",
    "train_imgs, _ = next(train_generator(len(ids_train_split[4000:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#print val_imgs.shape\n",
    "print train_imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#val_pred_masks = model.predict(val_imgs, batch_size=16, verbose=1)\n",
    "train_pred_masks = model.predict(train_imgs, batch_size=16, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "preds = np.squeeze(train_pred_masks, axis=3)\n",
    "np.shape(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pred_mask_path = \"outputs/valid_masks/{}_mask.gif\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for i in range(train_imgs.shape[0]):\n",
    "    mask = preds[i]\n",
    "    mask = cv2.resize(mask, (1918, 1280), interpolation=cv2.INTER_LINEAR)\n",
    "    mask = (mask > 0.95)\n",
    "    mask = Image.fromarray((mask * 255).astype(np.uint8), mode='L')\n",
    "    mask.save(pred_mask_path.format(ids_valid_split.values[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def np_dice_value(y_true, y_pred):\n",
    "    smooth = 1.\n",
    "    y_true_f = y_true.flatten()\n",
    "    y_pred_f = y_pred.flatten()\n",
    "    intersection = np.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (np.sum(y_true_f) + np.sum(y_pred_f) + smooth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "run_name = 'unet-2017-09-03-1739'\n",
    "model.load_weights('weights/{}.hdf5'.format(run_name))\n",
    "\n",
    "val_imgs, val_masks = next(valid_generator(len(ids_valid_split)))\n",
    "val_imgs = np.array(val_imgs)\n",
    "val_masks = np.array(val_masks)\n",
    "val_pred_masks = model.predict(val_imgs, batch_size=1)\n",
    "masks_val_dices = [np_dice_value(mask, pred_mask) for (mask, pred_mask) in zip(val_masks, val_pred_masks)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Display the worst predicted mask for validation examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "index = np.argsort(masks_val_dices)[7]\n",
    "id = ids_valid_split.values[index]\n",
    "utils.show_mask(train_path.format(id), val_masks[index].squeeze(), val_pred_masks[index].squeeze(), show_img=False)\n",
    "print id, masks_val_dices[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "indices = np.argsort(masks_val_dices[masks_val_dices <= 99.6])\n",
    "for id in indices:\n",
    "        print(masks_val_dices[id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "hist, bins = np.histogram(masks_val_dices, bins=50)\n",
    "width = 0.7 * (bins[1] - bins[0])\n",
    "center = (bins[:-1] + bins[1:]) / 2\n",
    "plt.bar(center, hist, align='center', width=width)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "indices = np.random.randint(len(ids_valid_split), size=3)\n",
    "for index in indices:\n",
    "    id = ids_valid_split.values[index]\n",
    "    utils.show_mask(train_path.format(id), val_masks[index].squeeze(), val_pred_masks[index].squeeze(),\n",
    "                    show_img=True, bbox = bboxes[id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Create model first if required\n",
    "run_name = 'unet-2017-08-20-5'\n",
    "model.load_weights('weights/{}.hdf5'.format(run_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Generate Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "threshold = 0.5\n",
    "test_path = 'inputs/test1/' #'inputs/test/'\n",
    "test_masks_path = 'outputs/test1_masks/' #None\n",
    "generate_submit(model, input_size, batch_size, threshold, test_path, 'outputs/', run_name, test_masks_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "utils.show_test_masks(test_path, test_masks_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
